{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87856bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.setup.hf_auth import hf_login\n",
    "from src.utils.prompts import CONCISE_REWRITE_PROMPT_TEMPLATE, REFLECTION_PROMPT_TEMPLATE\n",
    "from src.utils.helpers import mask_explanation, mask_explanation_fa\n",
    "from src.evaluation.metrics import (\n",
    "    load_model, \n",
    "    calc_perplexity_single, \n",
    "    compute_conciseness_metrics,\n",
    ")\n",
    "from src.evaluation import scorer_logprobs, final_prediction\n",
    "\n",
    "def rewrite_explanation(explanation, rewrite_model):\n",
    "    \"\"\"Rewrite explanation using a language model and a concise prompt.\"\"\"\n",
    "    prompt = CONCISE_REWRITE_PROMPT_TEMPLATE.format(explanation=explanation)\n",
    "    # The rewrite_model must have a `.generate_text(prompt)` method (LLM API or local model)\n",
    "    return rewrite_model.generate_text(prompt)\n",
    "\n",
    "def reflect_on_explanation(explanation, reflection_model):\n",
    "    \"\"\"Optional: ask model to reflect on sufficiency and logic.\"\"\"\n",
    "    prompt = REFLECTION_PROMPT_TEMPLATE.format(explanation=explanation)\n",
    "    return reflection_model.generate_text(prompt)\n",
    "\n",
    "def process_mcqa_sample(sample, rewrite_model, scorer_model, scorer_tokenizer, logger, mask_fa=False, reflect=False):\n",
    "    \"\"\"\n",
    "    Pipeline for a single MCQA sample.\n",
    "    sample: dict with keys 'question', 'choices', 'answer', 'verbose_explanation' (and *_fa for Farsi)\n",
    "    rewrite_model: model for rewriting explanations (e.g., GPT-4)\n",
    "    scorer_model, scorer_tokenizer: model/tokenizer for scoring answers\n",
    "    logger: logging.Logger\n",
    "    mask_fa: bool, use Farsi masking\n",
    "    reflect: bool, ask for meta-reflection\n",
    "    \"\"\"\n",
    "    # Step 1: Concise Rewrite\n",
    "    concise_explanation = rewrite_explanation(\n",
    "        sample[\"verbose_explanation\"], rewrite_model\n",
    "    )\n",
    "    logger.info(f\"Concise explanation: {concise_explanation}\")\n",
    "\n",
    "    # Step 2: Mask Choices (to avoid label leakage)\n",
    "    mask_func = mask_explanation_fa if mask_fa else mask_explanation\n",
    "    row = sample.copy()\n",
    "    row[\"explanation\"] = sample[\"verbose_explanation\"]\n",
    "    verbose_masked = mask_func(row)\n",
    "    row[\"explanation\"] = concise_explanation\n",
    "    concise_masked = mask_func(row)\n",
    "    logger.info(f\"Masked verbose: {verbose_masked}\")\n",
    "    logger.info(f\"Masked concise: {concise_masked}\")\n",
    "\n",
    "    # Step 3: Evaluate sufficiency (log-prob gap)\n",
    "    suff_delta_verbose = scorer_logprobs(\n",
    "        scorer_model, scorer_tokenizer, \n",
    "        sample[\"question\"], verbose_masked, sample[\"choices\"]\n",
    "    )\n",
    "    suff_delta_concise = scorer_logprobs(\n",
    "        scorer_model, scorer_tokenizer, \n",
    "        sample[\"question\"], concise_masked, sample[\"choices\"]\n",
    "    )\n",
    "    logger.info(f\"Sufficiency delta (verbose): {suff_delta_verbose}\")\n",
    "    logger.info(f\"Sufficiency delta (concise): {suff_delta_concise}\")\n",
    "\n",
    "    # Step 4: Measure conciseness\n",
    "    verbose_tokens, concise_tokens, reduction, percent = compute_conciseness_metrics(\n",
    "        sample[\"verbose_explanation\"], concise_explanation\n",
    "    )\n",
    "    logger.info(f\"Tokens: verbose={verbose_tokens}, concise={concise_tokens}, reduction={reduction}, percent={percent:.2f}\")\n",
    "\n",
    "    # Step 5: Final prediction using concise explanation\n",
    "    final_pred = final_prediction(\n",
    "        scorer_model, scorer_tokenizer, \n",
    "        sample[\"question\"], concise_explanation, sample[\"choices\"]\n",
    "    )\n",
    "    is_correct = (final_pred == sample[\"answer\"])\n",
    "    logger.info(f\"Final prediction with concise: {final_pred} (Correct? {is_correct})\")\n",
    "\n",
    "    # Step 6 (optional): Reflection\n",
    "    reflection = None\n",
    "    if reflect:\n",
    "        reflection = reflect_on_explanation(concise_explanation, rewrite_model)\n",
    "        logger.info(f\"Reflection: {reflection}\")\n",
    "\n",
    "    # Step 7: Perplexity (optional, if you want to log it)\n",
    "    verbose_ppl = calc_perplexity_single(sample[\"verbose_explanation\"], scorer_tokenizer, scorer_model, logger)\n",
    "    concise_ppl = calc_perplexity_single(concise_explanation, scorer_tokenizer, scorer_model, logger)\n",
    "\n",
    "    # Aggregate results\n",
    "    return {\n",
    "        \"concise_explanation\": concise_explanation,\n",
    "        \"verbose_masked\": verbose_masked,\n",
    "        \"concise_masked\": concise_masked,\n",
    "        \"suff_delta_verbose\": suff_delta_verbose,\n",
    "        \"suff_delta_concise\": suff_delta_concise,\n",
    "        \"verbose_tokens\": verbose_tokens,\n",
    "        \"concise_tokens\": concise_tokens,\n",
    "        \"token_reduction\": reduction,\n",
    "        \"percent_reduction\": percent,\n",
    "        \"final_pred\": final_pred,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"reflection\": reflection,\n",
    "        \"verbose_ppl\": verbose_ppl,\n",
    "        \"concise_ppl\": concise_ppl\n",
    "    }\n",
    "\n",
    "def process_batch(samples, rewrite_model, scorer_model, scorer_tokenizer, logger, mask_fa=False, reflect=False):\n",
    "    \"\"\"Process a batch of MCQA samples and collect results as a DataFrame.\"\"\"\n",
    "    import pandas as pd\n",
    "    results = []\n",
    "    for sample in samples:\n",
    "        result = process_mcqa_sample(\n",
    "            sample, rewrite_model, scorer_model, scorer_tokenizer, logger, mask_fa=mask_fa, reflect=reflect\n",
    "        )\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage:\n",
    "# logger = logging.getLogger(\"pipeline\")\n",
    "# hf_login()  # Authenticate to HuggingFace\n",
    "# scorer_tokenizer, scorer_model = load_model(\"YourScorerModelName\", logger)\n",
    "# batch_results = process_batch(samples, rewrite_model, scorer_model, scorer_tokenizer, logger)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
