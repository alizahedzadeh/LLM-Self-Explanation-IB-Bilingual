{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd965867",
   "metadata": {},
   "source": [
    "# MCQA Concise Explanation Evaluation Pipeline\n",
    "\n",
    "This notebook implements a comprehensive pipeline to evaluate the impact of concise explanation rewriting on Multiple-Choice Question Answering (MCQA) tasks.\n",
    "\n",
    "## **Pipeline Overview**\n",
    "\n",
    "For each MCQA sample, the following steps are performed:\n",
    "\n",
    "1. **Concise Explanation Generation**  \n",
    "   - Uses an LLM (e.g., GPT-4) to rewrite the original explanation in a more concise form, preserving logical reasoning and correctness.\n",
    "\n",
    "2. **Masking for Label Leakage Prevention**  \n",
    "   - Masks answer letters and choice texts in both the verbose and concise explanations to prevent label leakage.\n",
    "\n",
    "3. **Sufficiency Evaluation**  \n",
    "   - Assesses whether the (masked) explanation alone is sufficient for a model to select the correct answer, using log-probability calculations over answer choices.\n",
    "\n",
    "4. **Conciseness Measurement**  \n",
    "   - Calculates token counts and reduction percentage to quantify how much shorter the concise explanation is compared to the verbose version.\n",
    "\n",
    "5. **End-to-End Prediction**  \n",
    "   - Tests whether the model can still predict the correct answer when using the concise explanation.\n",
    "\n",
    "6. **Perplexity Analysis** (Optional)  \n",
    "   - Computes language model perplexity for both verbose and concise explanations as an additional measure of informativeness and clarity.\n",
    "\n",
    "7. **Reflection** (Optional)  \n",
    "   - Prompts the model to self-assess the sufficiency and completeness of the concise explanation.\n",
    "\n",
    "## **Key Metrics Reported**\n",
    "\n",
    "- **Sufficiency Delta:** Difference in log-probabilities between the correct answer and competing choices, for both verbose and concise explanations.\n",
    "- **Token Reduction:** Number and percentage of tokens saved by conciseness.\n",
    "- **Prediction Accuracy:** Whether the model selects the correct answer using the concise explanation.\n",
    "- **Perplexity:** Language model perplexity scores for each explanation.\n",
    "- **Reflection Output:** Model's self-assessment of explanation sufficiency (if enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcf496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87856bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Teias\\Thesis\\self-explaination-thesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONCISE_REWRITE_PROMPT_TEMPLATE, REFLECTION_PROMPT_TEMPLATE\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mask_explanation, mask_explanation_fa\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     load_model, \n\u001b[0;32m      7\u001b[0m     calc_perplexity_single, \n\u001b[0;32m      8\u001b[0m     compute_conciseness_metrics,\n\u001b[0;32m      9\u001b[0m     scorer_logprobs,\n\u001b[0;32m     10\u001b[0m     final_prediction\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrewrite_explanation\u001b[39m(explanation, rewrite_model):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Rewrite explanation using a language model and a concise prompt.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Teias\\Thesis\\self-explaination-thesis\\src\\evaluation\\metrics.py:115\u001b[0m\n\u001b[0;32m    111\u001b[0m     print_regression_report(y_true_reg, y_pred_reg)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from src.setup.hf_auth import hf_login\n",
    "from src.utils.prompts import CONCISE_REWRITE_PROMPT_TEMPLATE, REFLECTION_PROMPT_TEMPLATE\n",
    "from src.utils.helpers import mask_explanation, mask_explanation_fa\n",
    "from src.evaluation.metrics import (\n",
    "    load_model, \n",
    "    calc_perplexity_single, \n",
    "    compute_conciseness_metrics,\n",
    "    scorer_logprobs,\n",
    "    final_prediction\n",
    ")\n",
    "\n",
    "def rewrite_explanation(explanation, rewrite_model):\n",
    "    \"\"\"Rewrite explanation using a language model and a concise prompt.\"\"\"\n",
    "    prompt = CONCISE_REWRITE_PROMPT_TEMPLATE.format(explanation=explanation)\n",
    "    # The rewrite_model must have a `.generate_text(prompt)` method (LLM API or local model)\n",
    "    return rewrite_model.generate_text(prompt)\n",
    "\n",
    "def reflect_on_explanation(explanation, reflection_model):\n",
    "    \"\"\"Optional: ask model to reflect on sufficiency and logic.\"\"\"\n",
    "    prompt = REFLECTION_PROMPT_TEMPLATE.format(explanation=explanation)\n",
    "    return reflection_model.generate_text(prompt)\n",
    "\n",
    "def process_mcqa_sample(sample, rewrite_model, scorer_model, scorer_tokenizer, logger, mask_fa=False, reflect=False):\n",
    "    \"\"\"\n",
    "    Pipeline for a single MCQA sample.\n",
    "    sample: dict with keys 'question', 'choices', 'answer', 'verbose_explanation' (and *_fa for Farsi)\n",
    "    rewrite_model: model for rewriting explanations (e.g., GPT-4)\n",
    "    scorer_model, scorer_tokenizer: model/tokenizer for scoring answers\n",
    "    logger: logging.Logger\n",
    "    mask_fa: bool, use Farsi masking\n",
    "    reflect: bool, ask for meta-reflection\n",
    "    \"\"\"\n",
    "    # Step 1: Concise Rewrite\n",
    "    concise_explanation = rewrite_explanation(\n",
    "        sample[\"verbose_explanation\"], rewrite_model\n",
    "    )\n",
    "    logger.info(f\"Concise explanation: {concise_explanation}\")\n",
    "\n",
    "    # Step 2: Mask Choices (to avoid label leakage)\n",
    "    mask_func = mask_explanation_fa if mask_fa else mask_explanation\n",
    "    row = sample.copy()\n",
    "    row[\"explanation\"] = sample[\"verbose_explanation\"]\n",
    "    verbose_masked = mask_func(row)\n",
    "    row[\"explanation\"] = concise_explanation\n",
    "    concise_masked = mask_func(row)\n",
    "    logger.info(f\"Masked verbose: {verbose_masked}\")\n",
    "    logger.info(f\"Masked concise: {concise_masked}\")\n",
    "\n",
    "    # Step 3: Evaluate sufficiency (log-prob gap)\n",
    "    suff_delta_verbose = scorer_logprobs(\n",
    "        scorer_model, scorer_tokenizer, \n",
    "        sample[\"question\"], verbose_masked, sample[\"choices\"]\n",
    "    )\n",
    "    suff_delta_concise = scorer_logprobs(\n",
    "        scorer_model, scorer_tokenizer, \n",
    "        sample[\"question\"], concise_masked, sample[\"choices\"]\n",
    "    )\n",
    "    logger.info(f\"Sufficiency delta (verbose): {suff_delta_verbose}\")\n",
    "    logger.info(f\"Sufficiency delta (concise): {suff_delta_concise}\")\n",
    "\n",
    "    # Step 4: Measure conciseness\n",
    "    verbose_tokens, concise_tokens, reduction, percent = compute_conciseness_metrics(\n",
    "        sample[\"verbose_explanation\"], concise_explanation\n",
    "    )\n",
    "    logger.info(f\"Tokens: verbose={verbose_tokens}, concise={concise_tokens}, reduction={reduction}, percent={percent:.2f}\")\n",
    "\n",
    "    # Step 5: Final prediction using concise explanation\n",
    "    final_pred = final_prediction(\n",
    "        scorer_model, scorer_tokenizer, \n",
    "        sample[\"question\"], concise_explanation, sample[\"choices\"]\n",
    "    )\n",
    "    is_correct = (final_pred == sample[\"answer\"])\n",
    "    logger.info(f\"Final prediction with concise: {final_pred} (Correct? {is_correct})\")\n",
    "\n",
    "    # Step 6 (optional): Reflection\n",
    "    reflection = None\n",
    "    if reflect:\n",
    "        reflection = reflect_on_explanation(concise_explanation, rewrite_model)\n",
    "        logger.info(f\"Reflection: {reflection}\")\n",
    "\n",
    "    # Step 7: Perplexity (optional, if you want to log it)\n",
    "    verbose_ppl = calc_perplexity_single(sample[\"verbose_explanation\"], scorer_tokenizer, scorer_model, logger)\n",
    "    concise_ppl = calc_perplexity_single(concise_explanation, scorer_tokenizer, scorer_model, logger)\n",
    "\n",
    "    # Aggregate results\n",
    "    return {\n",
    "        \"concise_explanation\": concise_explanation,\n",
    "        \"verbose_masked\": verbose_masked,\n",
    "        \"concise_masked\": concise_masked,\n",
    "        \"suff_delta_verbose\": suff_delta_verbose,\n",
    "        \"suff_delta_concise\": suff_delta_concise,\n",
    "        \"verbose_tokens\": verbose_tokens,\n",
    "        \"concise_tokens\": concise_tokens,\n",
    "        \"token_reduction\": reduction,\n",
    "        \"percent_reduction\": percent,\n",
    "        \"final_pred\": final_pred,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"reflection\": reflection,\n",
    "        \"verbose_ppl\": verbose_ppl,\n",
    "        \"concise_ppl\": concise_ppl\n",
    "    }\n",
    "\n",
    "def process_batch(samples, rewrite_model, scorer_model, scorer_tokenizer, logger, mask_fa=False, reflect=False):\n",
    "    \"\"\"Process a batch of MCQA samples and collect results as a DataFrame.\"\"\"\n",
    "    import pandas as pd\n",
    "    results = []\n",
    "    for sample in samples:\n",
    "        result = process_mcqa_sample(\n",
    "            sample, rewrite_model, scorer_model, scorer_tokenizer, logger, mask_fa=mask_fa, reflect=reflect\n",
    "        )\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage:\n",
    "# logger = logging.getLogger(\"pipeline\")\n",
    "# hf_login()  # Authenticate to HuggingFace\n",
    "# scorer_tokenizer, scorer_model = load_model(\"YourScorerModelName\", logger)\n",
    "# batch_results = process_batch(samples, rewrite_model, scorer_model, scorer_tokenizer, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689973be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "class OpenRouterModel:\n",
    "    def __init__(self, api_key, model_name, base_url=\"https://openrouter.ai/api/v1/chat\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Title\": \"ConciseExplanationProject\"       # Optional, appears in dashboard\n",
    "        }\n",
    "        self.default_temperature = 0.3\n",
    "        self.default_max_tokens = 512\n",
    "\n",
    "    def set_model(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def set_temperature(self, temperature: float):\n",
    "        self.default_temperature = temperature\n",
    "\n",
    "    def set_token_limit(self, max_tokens: int):\n",
    "        self.default_max_tokens = max_tokens\n",
    "\n",
    "    def chat(self, prompt: str, system_prompt: str = \"\", temperature: float = None, max_tokens: int = None, retries: int = 3, sleep_between_retries: float = 2.0) -> str:\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [],\n",
    "            \"temperature\": temperature if temperature is not None else self.default_temperature,\n",
    "            \"max_tokens\": max_tokens if max_tokens is not None else self.default_max_tokens,\n",
    "        }\n",
    "\n",
    "        if system_prompt:\n",
    "            payload[\"messages\"].append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        payload[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                response = requests.post(self.base_url + \"/completions\", headers=self.headers, json=payload)\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                else:\n",
    "                    raise ValueError(f\"[OpenRouter Error] {response.status_code}: {response.text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Retry {attempt + 1}/{retries} failed: {e}\")\n",
    "                time.sleep(sleep_between_retries)\n",
    "                attempt += 1\n",
    "\n",
    "        raise RuntimeError(f\"OpenRouter request failed after {retries} retries.\")\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        return self.chat(prompt=prompt, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4844126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called **Rayleigh scattering**. Here's how it works:\n",
      "\n",
      "1. **Sunlight and the atmosphere:** Sunlight, or white light, is made up of many colors, each with different wavelengths. When sunlight enters Earth's atmosphere, it interacts with the molecules and small particles in the air.\n",
      "\n",
      "2. **Scattering of light:** These molecules scatter the sunlight in different directions. However, shorter wavelengths of light (blue and violet) are scattered much more strongly than longer wavelengths (red, orange, yellow).\n",
      "\n",
      "3. **Why blue and not violet?:** Although violet light is scattered even more than blue light, our eyes are more sensitive to blue, and some of the violet light is absorbed by the upper atmosphere, making the sky appear predominantly blue.\n",
      "\n",
      "4. **Effect during sunrise and sunset:** When the Sun is low on the horizon, sunlight travels through more atmosphere, scattering away most of the blue light and leaving reds and oranges, which is why sunrises and sunsets often appear red or orange.\n",
      "\n",
      "In summary, the sky is blue because the shorter blue wavelengths of sunlight are scattered in all directions by the molecules in the atmosphere, and this scattered blue light is what we see when we look up.\n",
      "Mitochondria generate cellular energy via respiration.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance\n",
    "model = OpenRouterModel(\n",
    "    api_key=\"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\",\n",
    "    model_name=\"openai/gpt-4.1-mini\"\n",
    ")\n",
    "\n",
    "# Simple usage\n",
    "response = model.generate(\"Explain why the sky is blue.\")\n",
    "print(response)\n",
    "\n",
    "# Custom parameters and system message\n",
    "response = model.chat(\n",
    "    prompt=\"Rewrite this explanation concisely: The mitochondria produces energy in cells through respiration.\",\n",
    "    system_prompt=\"You're a helpful assistant that rewrites verbose explanations more concisely.\",\n",
    "    temperature=0.3\n",
    ")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
