{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272c293a",
   "metadata": {},
   "source": [
    "# Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a582a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean.csv\")  # Replace with actual file path\n",
    "\n",
    "# Split into 3 chunks\n",
    "chunk1 = df.iloc[:1000]\n",
    "chunk2 = df.iloc[1000:2000]\n",
    "chunk3 = df.iloc[2000:]\n",
    "\n",
    "# Optionally save each chunk\n",
    "chunk1.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk1.csv\", index=False)\n",
    "chunk2.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk2.csv\", index=False)\n",
    "chunk3.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fa79d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 00:09:12,225 - INFO - setup_logging:31 - Logging initialized. Log file: logs\\20250604_000912_deepseek_v3_predict.log\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Any, Optional, List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging(log_file: str = \"deepseek_v3_predict.log\"):\n",
    "    \"\"\"Setup comprehensive logging configuration\"\"\"\n",
    "    log_dir = Path(\"logs\")\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    log_path = log_dir / f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{log_file}\"\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Logging initialized. Log file: {log_path}\")\n",
    "    return logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff54879",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdf2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for the DeepSeek V3 prediction system\"\"\"\n",
    "    # OpenRouter API configuration\n",
    "    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', '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')\n",
    "    MODEL = \"deepseek/deepseek-chat:free\"  # DeepSeek V3 model on OpenRouter\n",
    "    TEMPERATURE = 0.0\n",
    "    MAX_RETRIES = 3\n",
    "    RETRY_DELAY = 1.0  # seconds\n",
    "    REQUEST_DELAY = 0.5  # delay between requests to avoid rate limiting\n",
    "    TIMEOUT = 30  # seconds\n",
    "    MAX_TOKENS = 1000\n",
    "\n",
    "# Initialize OpenRouter client (compatible with OpenAI client)\n",
    "client = OpenAI(\n",
    "    api_key=Config.OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19450642",
   "metadata": {},
   "source": [
    "# Updated prompt for questions without explicit passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa05792",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_then_explain_prompt_context = '''\n",
    "You are given a multiple-choice question. \n",
    "\n",
    "Step 1: Based on your knowledge and reasoning, select the most likely correct answer.  \n",
    "Step 2: Justify your answer with clear reasoning and explanation.\n",
    "\n",
    "Instructions:\n",
    "- Use logical reasoning to determine the best answer.\n",
    "- Do not reference the other answer options in your explanation.\n",
    "- Keep the explanation concise but informative (2-4 sentences).\n",
    "- Provide clear reasoning for your choice.\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "A) {option_A}  \n",
    "B) {option_B}  \n",
    "C) {option_C}  \n",
    "D) {option_D}\n",
    "\n",
    "Respond in this format:\n",
    "\n",
    "<prediction>A/B/C/D</prediction>  \n",
    "<explanation>[Your reasoning and justification for the answer]</explanation>\n",
    "'''\n",
    "\n",
    "def validate_row(row: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate that a row has all required fields for your data structure\"\"\"\n",
    "    required_fields = ['question', 'choice_A', 'choice_B', 'choice_C', 'choice_D']\n",
    "    missing_fields = [field for field in required_fields if field not in row or pd.isna(row[field]) or not str(row[field]).strip()]\n",
    "    \n",
    "    if missing_fields:\n",
    "        logger.warning(f\"Row missing required fields: {missing_fields}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_prediction_and_explanation(output: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Extract prediction and explanation from model output using regex\"\"\"\n",
    "    try:   \n",
    "        logger.info(output)\n",
    "        # Extract prediction\n",
    "        pred_match = re.search(r\"<prediction>\\s*([A-D])\\s*</prediction>\", output, re.IGNORECASE)\n",
    "        prediction = pred_match.group(1).upper() if pred_match else None\n",
    "        \n",
    "        # Extract explanation\n",
    "        expl_match = re.search(r\"<explanation>\\s*(.*?)\\s*</explanation>\", output, re.DOTALL | re.IGNORECASE)\n",
    "        explanation = expl_match.group(1).strip() if expl_match else None\n",
    "        \n",
    "        if not prediction:\n",
    "            logger.warning(\"Could not extract prediction from output\")\n",
    "        if not explanation:\n",
    "            logger.warning(\"Could not extract explanation from output\")\n",
    "            \n",
    "        return {\n",
    "            \"prediction\": prediction,\n",
    "            \"explanation\": explanation\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting prediction/explanation: {e}\")\n",
    "        return {\"prediction\": None, \"explanation\": None}\n",
    "\n",
    "def make_api_call(prompt: str, row_id: Optional[int] = None) -> Optional[str]:\n",
    "    \"\"\"Make API call with retry logic and error handling using OpenRouter client\"\"\"\n",
    "    for attempt in range(Config.MAX_RETRIES):\n",
    "        try:\n",
    "            logger.debug(f\"Making API call (attempt {attempt + 1}/{Config.MAX_RETRIES}) for row {row_id}\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=Config.MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=Config.TEMPERATURE,\n",
    "                max_tokens=Config.MAX_TOKENS,\n",
    "                timeout=Config.TIMEOUT,\n",
    "            )\n",
    "            \n",
    "            output = response.choices[0].message.content\n",
    "            print(\"MODEL OUTPUT :\", output)\n",
    "            # Log token usage\n",
    "            if hasattr(response, 'usage') and response.usage:\n",
    "                usage = response.usage\n",
    "                logger.info(f\"Row {row_id} - Tokens used: {usage.total_tokens} \"\n",
    "                           f\"(prompt: {usage.prompt_tokens}, \"\n",
    "                           f\"completion: {usage.completion_tokens})\")\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Handle different types of errors\n",
    "            if \"rate_limit\" in error_msg.lower() or \"429\" in error_msg:\n",
    "                wait_time = Config.RETRY_DELAY * (2 ** attempt)  # Exponential backoff\n",
    "                logger.warning(f\"Rate limit hit for row {row_id}, waiting {wait_time}s before retry {attempt + 1}\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "            elif \"timeout\" in error_msg.lower():\n",
    "                logger.error(f\"API timeout for row {row_id}: {error_msg}\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(Config.RETRY_DELAY)\n",
    "                    \n",
    "            elif \"api\" in error_msg.lower():\n",
    "                logger.error(f\"OpenRouter API error for row {row_id}: {error_msg}\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(Config.RETRY_DELAY)\n",
    "                    \n",
    "            else:\n",
    "                logger.error(f\"Unexpected error ({error_type}) for row {row_id}: {error_msg}\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(Config.RETRY_DELAY)\n",
    "    \n",
    "    logger.error(f\"Failed to get response for row {row_id} after {Config.MAX_RETRIES} attempts\")\n",
    "    return None\n",
    "\n",
    "def deepseek_v3_predict(row: Dict[str, Any], row_id: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced DeepSeek V3 prediction function adapted for your data structure\n",
    "    \n",
    "    Args:\n",
    "        row: Dictionary containing question data with columns: question, choice_A, choice_B, choice_C, choice_D\n",
    "        row_id: Optional row identifier for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting prediction for row {row_id}, question ID: {row.get('id', 'N/A')}\")\n",
    "    \n",
    "    # Validate input\n",
    "    if not validate_row(row):\n",
    "        logger.error(f\"Row {row_id} validation failed\")\n",
    "        return {\n",
    "            \"row_id\": row_id,\n",
    "            \"question_id\": row.get('id'),\n",
    "            \"prediction\": None,\n",
    "            \"explanation\": None,\n",
    "            \"raw_output\": None,\n",
    "            \"actual_answer\": row.get('answerKey'),\n",
    "            \"is_correct\": None,\n",
    "            \"error\": \"Invalid input data\",\n",
    "            \"processing_time\": 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Format prompt using your column names\n",
    "        prompt = predict_then_explain_prompt_context.format(\n",
    "            question=row['question'],\n",
    "            option_A=row['choice_A'],\n",
    "            option_B=row['choice_B'],\n",
    "            option_C=row['choice_C'],\n",
    "            option_D=row['choice_D'],\n",
    "        )\n",
    "        \n",
    "        logger.debug(f\"Row {row_id} - Prompt length: {len(prompt)} characters\")\n",
    "        \n",
    "        # Make API call\n",
    "        output = make_api_call(prompt, row_id)\n",
    "        logger.info(output)\n",
    "        if output is None:\n",
    "            return {\n",
    "                \"row_id\": row_id,\n",
    "                \"question_id\": row.get('id'),\n",
    "                \"prediction\": None,\n",
    "                \"explanation\": None,\n",
    "                \"raw_output\": None,\n",
    "                \"actual_answer\": row.get('answerKey'),\n",
    "                \"is_correct\": None,\n",
    "                \"error\": \"API call failed\",\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # Extract prediction and explanation\n",
    "        extracted = extract_prediction_and_explanation(output)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        actual_answer = row.get('answerKey')\n",
    "        predicted_answer = extracted[\"prediction\"]\n",
    "        is_correct = predicted_answer == actual_answer if (predicted_answer and actual_answer) else None\n",
    "        \n",
    "        result = {\n",
    "            \"row_id\": row_id,\n",
    "            \"question_id\": row.get('id'),\n",
    "            \"prediction\": predicted_answer,\n",
    "            \"explanation\": extracted[\"explanation\"],\n",
    "            \"raw_output\": output,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"split\": row.get('split'),\n",
    "            \"error\": None,\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Row {row_id} completed successfully in {processing_time:.2f}s - \"\n",
    "                   f\"Prediction: {result['prediction']}, Actual: {actual_answer}, \"\n",
    "                   f\"Correct: {is_correct}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.error(f\"Unexpected error processing row {row_id}: {e}\")\n",
    "        return {\n",
    "            \"row_id\": row_id,\n",
    "            \"question_id\": row.get('id'),\n",
    "            \"prediction\": None,\n",
    "            \"explanation\": None,\n",
    "            \"raw_output\": output,\n",
    "            \"actual_answer\": row.get('answerKey'),\n",
    "            \"is_correct\": None,\n",
    "            \"error\": str(e),\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, \n",
    "                     save_interval: int = 50,\n",
    "                     output_file: str = \"deepseek_v3_predictions_results.json\",\n",
    "                     start_index: int = 0,\n",
    "                     end_index: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a pandas DataFrame with your specific column structure\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns: question, choice_A, choice_B, choice_C, choice_D, answerKey, etc.\n",
    "        save_interval: Save results every N processed items\n",
    "        output_file: File to save results to\n",
    "        start_index: Start processing from this index (useful for resuming)\n",
    "        end_index: Stop processing at this index (useful for testing subsets)\n",
    "        \n",
    "    Returns:\n",
    "        List of prediction results\n",
    "    \"\"\"\n",
    "    if end_index is None:\n",
    "        end_index = len(df)\n",
    "    \n",
    "    subset_df = df.iloc[start_index:end_index]\n",
    "    \n",
    "    logger.info(f\"Starting batch processing of {len(subset_df)} items (rows {start_index} to {end_index-1})\")\n",
    "    logger.info(f\"DataFrame info: {len(df)} total rows, columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Log data distribution\n",
    "    if 'split' in df.columns:\n",
    "        split_counts = df['split'].value_counts()\n",
    "        logger.info(f\"Data split distribution: {split_counts.to_dict()}\")\n",
    "    \n",
    "    if 'answerKey' in df.columns:\n",
    "        answer_dist = df['answerKey'].value_counts()\n",
    "        logger.info(f\"Answer distribution: {answer_dist.to_dict()}\")\n",
    "    \n",
    "    results = []\n",
    "    failed_count = 0\n",
    "    correct_count = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(\n",
    "        subset_df.iterrows(), \n",
    "        total=len(subset_df),\n",
    "        desc=\"Processing questions with DeepSeek V3\",\n",
    "        unit=\"question\",\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for original_idx, row in pbar:\n",
    "            # Process the row\n",
    "            result = deepseek_v3_predict(row.to_dict(), row_id=original_idx)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update counters\n",
    "            if result['prediction'] is None:\n",
    "                failed_count += 1\n",
    "            elif result['is_correct'] is True:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Update progress bar description\n",
    "            total_processed = len(results)\n",
    "            success_rate = ((total_processed - failed_count) / total_processed * 100) if total_processed > 0 else 0\n",
    "            accuracy = (correct_count / (total_processed - failed_count) * 100) if (total_processed - failed_count) > 0 else 0\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Success': f'{success_rate:.1f}%',\n",
    "                'Accuracy': f'{accuracy:.1f}%',\n",
    "                'Last': result['prediction'] or 'FAIL'\n",
    "            })\n",
    "            \n",
    "            # Periodic saving\n",
    "            if len(results) % save_interval == 0:\n",
    "                save_results(results, output_file)\n",
    "                logger.info(f\"Intermediate save completed at {len(results)} processed items\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(Config.REQUEST_DELAY)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Processing interrupted by user\")\n",
    "        pbar.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during batch processing: {e}\")\n",
    "        pbar.close()\n",
    "        \n",
    "    finally:\n",
    "        # Final save\n",
    "        save_results(results, output_file)\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_processed = len(results)\n",
    "        successful = sum(1 for r in results if r['prediction'] is not None)\n",
    "        correct = sum(1 for r in results if r['is_correct'] is True)\n",
    "        success_rate = successful / total_processed * 100 if total_processed > 0 else 0\n",
    "        accuracy = correct / successful * 100 if successful > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Batch processing completed:\")\n",
    "        logger.info(f\"  Total processed: {total_processed}\")\n",
    "        logger.info(f\"  Successful predictions: {successful}\")\n",
    "        logger.info(f\"  Failed predictions: {total_processed - successful}\")\n",
    "        logger.info(f\"  Correct answers: {correct}\")\n",
    "        logger.info(f\"  Success rate: {success_rate:.2f}%\")\n",
    "        logger.info(f\"  Accuracy rate: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Split-wise accuracy if available\n",
    "        if successful > 0:\n",
    "            splits_accuracy = {}\n",
    "            for split_name in df['split'].unique() if 'split' in df.columns else ['all']:\n",
    "                split_results = [r for r in results if r.get('split') == split_name or split_name == 'all']\n",
    "                split_correct = sum(1 for r in split_results if r['is_correct'] is True)\n",
    "                split_successful = sum(1 for r in split_results if r['prediction'] is not None)\n",
    "                if split_successful > 0:\n",
    "                    splits_accuracy[split_name] = split_correct / split_successful * 100\n",
    "            \n",
    "            logger.info(f\"Accuracy by split: {splits_accuracy}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str):\n",
    "    \"\"\"Save results to JSON file with error handling\"\"\"\n",
    "    try:\n",
    "        output_path = Path(\"output\") \n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        full_path = output_path / f\"{timestamp}_{filename}\"\n",
    "        \n",
    "        with open(full_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        logger.info(f\"Results saved to {full_path}\")\n",
    "        \n",
    "        # Also save a summary CSV for easy analysis\n",
    "        summary_path = output_path / f\"{timestamp}_deepseek_v3_summary.csv\"\n",
    "        summary_df = pd.DataFrame([\n",
    "            {\n",
    "                'row_id': r['row_id'],\n",
    "                'question_id': r['question_id'],\n",
    "                'prediction': r['prediction'],\n",
    "                'actual_answer': r['actual_answer'],\n",
    "                'is_correct': r['is_correct'],\n",
    "                'split': r.get('split'),\n",
    "                'has_error': r['error'] is not None,\n",
    "                'processing_time': r['processing_time']\n",
    "            }\n",
    "            for r in results\n",
    "        ])\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        logger.info(f\"Summary saved to {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "\n",
    "def analyze_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Analyze and print results statistics\"\"\"\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEEPSEEK V3 ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total = len(df_results)\n",
    "    successful = len(df_results[df_results['prediction'].notna()])\n",
    "    correct = len(df_results[df_results['is_correct'] == True])\n",
    "    \n",
    "    print(f\"Total questions processed: {total}\")\n",
    "    print(f\"Successful predictions: {successful} ({successful/total*100:.1f}%)\")\n",
    "    if successful > 0:\n",
    "        print(f\"Correct predictions: {correct} ({correct/successful*100:.1f}% accuracy)\")\n",
    "    else:\n",
    "        print(\"Correct predictions: 0 (No successful predictions)\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = df_results[df_results['error'].notna()]\n",
    "    if len(errors) > 0:\n",
    "        print(f\"\\nErrors encountered: {len(errors)}\")\n",
    "        error_types = errors['error'].value_counts()\n",
    "        for error_type, count in error_types.items():\n",
    "            print(f\"  {error_type}: {count}\")\n",
    "    \n",
    "    # Answer distribution\n",
    "    if 'actual_answer' in df_results.columns:\n",
    "        print(f\"\\nActual answer distribution:\")\n",
    "        print(df_results['actual_answer'].value_counts().sort_index())\n",
    "    \n",
    "    if successful > 0 and 'prediction' in df_results.columns:\n",
    "        print(f\"\\nPredicted answer distribution:\")\n",
    "        pred_dist = df_results[df_results['prediction'].notna()]['prediction'].value_counts().sort_index()\n",
    "        print(pred_dist)\n",
    "    \n",
    "    # Split analysis if available\n",
    "    if 'split' in df_results.columns and successful > 0:\n",
    "        print(f\"\\nAccuracy by split:\")\n",
    "        for split in df_results['split'].unique():\n",
    "            if pd.notna(split):\n",
    "                split_df = df_results[df_results['split'] == split]\n",
    "                split_correct = len(split_df[split_df['is_correct'] == True])\n",
    "                split_total = len(split_df[split_df['prediction'].notna()])\n",
    "                if split_total > 0:\n",
    "                    accuracy = split_correct / split_total * 100\n",
    "                    print(f\"  {split.upper()}: {split_correct}/{split_total} ({accuracy:.1f}%)\")\n",
    "\n",
    "# Test API connection\n",
    "def test_api_connection():\n",
    "    \"\"\"Test if the OpenRouter API is working properly with DeepSeek V3\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Testing OpenRouter DeepSeek V3 API connection...\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=Config.MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello, please respond with 'DeepSeek V3 API working'.\"}],\n",
    "            max_tokens=10,\n",
    "            temperature=0,\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": \"https://github.com/alizahedzadeh\",\n",
    "                \"X-Title\": \"DeepSeek V3 API Test\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        logger.info(f\"API test successful. Response: {result}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"API test failed: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94f8db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = chunk3.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6afdd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 01:13:16,383 - INFO - test_api_connection:427 - Testing OpenRouter DeepSeek V3 API connection...\n",
      "2025-06-04 01:13:20,337 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 01:13:20,609 - INFO - test_api_connection:441 - API test successful. Response: DeepSeek V3 API working.\n",
      "2025-06-04 01:13:20,640 - INFO - <module>:28 - Starting full dataset processing with DeepSeek V3 by user: alizahedzadeh\n",
      "2025-06-04 01:13:20,640 - INFO - <module>:29 - Dataset size: 5 questions\n",
      "2025-06-04 01:13:20,640 - INFO - <module>:30 - Processing start time (UTC): 2025-06-04 01:13:20\n",
      "2025-06-04 01:13:20,644 - INFO - process_dataframe:241 - Starting batch processing of 5 items (rows 0 to 4)\n",
      "2025-06-04 01:13:20,645 - INFO - process_dataframe:242 - DataFrame info: 5 total rows, columns: ['Unnamed: 0.1', 'Unnamed: 0', 'id', 'question', 'answerKey', 'choice_A', 'choice_B', 'choice_C', 'choice_D', 'split']\n",
      "2025-06-04 01:13:20,647 - INFO - process_dataframe:247 - Data split distribution: {'validation': 3, 'test': 2}\n",
      "2025-06-04 01:13:20,648 - INFO - process_dataframe:251 - Answer distribution: {'A': 2, 'C': 1, 'D': 1, 'B': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5 entries, 2576 to 2432\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  5 non-null      int64 \n",
      " 1   Unnamed: 0    5 non-null      int64 \n",
      " 2   id            5 non-null      object\n",
      " 3   question      5 non-null      object\n",
      " 4   answerKey     5 non-null      object\n",
      " 5   choice_A      5 non-null      object\n",
      " 6   choice_B      5 non-null      object\n",
      " 7   choice_C      5 non-null      object\n",
      " 8   choice_D      5 non-null      object\n",
      " 9   split         5 non-null      object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 440.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5 entries, 2576 to 2432\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  5 non-null      int64 \n",
      " 1   Unnamed: 0    5 non-null      int64 \n",
      " 2   id            5 non-null      object\n",
      " 3   question      5 non-null      object\n",
      " 4   answerKey     5 non-null      object\n",
      " 5   choice_A      5 non-null      object\n",
      " 6   choice_B      5 non-null      object\n",
      " 7   choice_C      5 non-null      object\n",
      " 8   choice_D      5 non-null      object\n",
      " 9   split         5 non-null      object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 440.0+ bytes\n",
      "üöÄ Starting full dataset processing with DeepSeek V3...\n",
      "üìä Total questions: 5\n",
      "‚è∞ Start time: 2025-06-04 01:13:20\n",
      "üì¶ Processing in 1 chunks of 500 questions each\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing Chunk 1/1 with DeepSeek V3\n",
      "üìç Rows 0 to 4 (5 questions)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with DeepSeek V3:   0%|          | 0/5 [00:00<?, ?question/s]2025-06-04 01:13:20,648 - INFO - deepseek_v3_predict:129 - Starting prediction for row 2576, question ID: Mercury_7090598\n",
      "2025-06-04 01:13:21,787 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 01:13:22,584 - INFO - make_api_call:83 - Row 2576 - Tokens used: 225 (prompt: 164, completion: 61)\n",
      "2025-06-04 01:13:22,584 - INFO - deepseek_v3_predict:160 - <prediction>C</prediction>  \n",
      "<explanation>Subduction involves the movement of one tectonic plate beneath another, which generates significant heat due to friction and the melting of rock in the Earth's mantle. This process results in the transfer of a large amount of thermal energy.</explanation>\n",
      "2025-06-04 01:13:22,584 - INFO - extract_prediction_and_explanation:42 - <prediction>C</prediction>  \n",
      "<explanation>Subduction involves the movement of one tectonic plate beneath another, which generates significant heat due to friction and the melting of rock in the Earth's mantle. This process results in the transfer of a large amount of thermal energy.</explanation>\n",
      "2025-06-04 01:13:22,584 - INFO - deepseek_v3_predict:197 - Row 2576 completed successfully in 1.94s - Prediction: C, Actual: C, Correct: True\n",
      "Processing questions with DeepSeek V3:   0%|          | 0/5 [00:01<?, ?question/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT : <prediction>C</prediction>  \n",
      "<explanation>Subduction involves the movement of one tectonic plate beneath another, which generates significant heat due to friction and the melting of rock in the Earth's mantle. This process results in the transfer of a large amount of thermal energy.</explanation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with DeepSeek V3:  20%|‚ñà‚ñà        | 1/5 [00:02<00:08,  2.24s/question]2025-06-04 01:13:22,891 - INFO - deepseek_v3_predict:129 - Starting prediction for row 2278, question ID: Mercury_7108990\n",
      "2025-06-04 01:13:24,159 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 01:13:25,158 - INFO - make_api_call:83 - Row 2278 - Tokens used: 237 (prompt: 163, completion: 74)\n",
      "2025-06-04 01:13:25,158 - INFO - deepseek_v3_predict:160 - <prediction>A</prediction>  \n",
      "<explanation>A galaxy occupies the greatest amount of space because it is a massive collection of stars, stellar remnants, interstellar gas, dust, and dark matter, spanning hundreds of thousands of light-years in diameter. In comparison, a black hole, neutron star, or solar system is significantly smaller in scale.</explanation>\n",
      "2025-06-04 01:13:25,158 - INFO - extract_prediction_and_explanation:42 - <prediction>A</prediction>  \n",
      "<explanation>A galaxy occupies the greatest amount of space because it is a massive collection of stars, stellar remnants, interstellar gas, dust, and dark matter, spanning hundreds of thousands of light-years in diameter. In comparison, a black hole, neutron star, or solar system is significantly smaller in scale.</explanation>\n",
      "2025-06-04 01:13:25,158 - INFO - deepseek_v3_predict:197 - Row 2278 completed successfully in 2.27s - Prediction: A, Actual: A, Correct: True\n",
      "Processing questions with DeepSeek V3:  20%|‚ñà‚ñà        | 1/5 [00:04<00:08,  2.24s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT : <prediction>A</prediction>  \n",
      "<explanation>A galaxy occupies the greatest amount of space because it is a massive collection of stars, stellar remnants, interstellar gas, dust, and dark matter, spanning hundreds of thousands of light-years in diameter. In comparison, a black hole, neutron star, or solar system is significantly smaller in scale.</explanation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with DeepSeek V3:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:04<00:07,  2.44s/question]2025-06-04 01:13:25,474 - INFO - deepseek_v3_predict:129 - Starting prediction for row 2104, question ID: Mercury_184818\n",
      "2025-06-04 01:13:26,752 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 01:13:30,423 - INFO - make_api_call:83 - Row 2104 - Tokens used: 253 (prompt: 182, completion: 71)\n",
      "2025-06-04 01:13:30,423 - INFO - deepseek_v3_predict:160 - <prediction>D</prediction>  \n",
      "<explanation>In a higher gravitational field, an organism would need to support more weight, requiring stronger limbs to maintain mobility and stability. Shorter limbs would reduce the strain on the organism by decreasing the leverage and stress on joints, making short, strong limbs the most beneficial adaptation.</explanation>\n",
      "2025-06-04 01:13:30,423 - INFO - extract_prediction_and_explanation:42 - <prediction>D</prediction>  \n",
      "<explanation>In a higher gravitational field, an organism would need to support more weight, requiring stronger limbs to maintain mobility and stability. Shorter limbs would reduce the strain on the organism by decreasing the leverage and stress on joints, making short, strong limbs the most beneficial adaptation.</explanation>\n",
      "2025-06-04 01:13:30,423 - INFO - deepseek_v3_predict:197 - Row 2104 completed successfully in 4.95s - Prediction: D, Actual: D, Correct: True\n",
      "Processing questions with DeepSeek V3:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:09<00:07,  2.44s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT : <prediction>D</prediction>  \n",
      "<explanation>In a higher gravitational field, an organism would need to support more weight, requiring stronger limbs to maintain mobility and stability. Shorter limbs would reduce the strain on the organism by decreasing the leverage and stress on joints, making short, strong limbs the most beneficial adaptation.</explanation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with DeepSeek V3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:10<00:07,  3.73s/question]2025-06-04 01:13:30,730 - INFO - deepseek_v3_predict:129 - Starting prediction for row 2445, question ID: Mercury_SC_407314\n",
      "2025-06-04 01:13:32,452 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 01:13:33,350 - INFO - make_api_call:83 - Row 2445 - Tokens used: 257 (prompt: 199, completion: 58)\n",
      "2025-06-04 01:13:33,350 - INFO - deepseek_v3_predict:160 - <prediction>A</prediction>  \n",
      "<explanation>Reviewing the steps taken during the investigations allows students to identify any differences in methodology, equipment, or execution that could have led to the differing results. This approach directly addresses the root cause of the discrepancy.</explanation>\n",
      "2025-06-04 01:13:33,356 - INFO - extract_prediction_and_explanation:42 - <prediction>A</prediction>  \n",
      "<explanation>Reviewing the steps taken during the investigations allows students to identify any differences in methodology, equipment, or execution that could have led to the differing results. This approach directly addresses the root cause of the discrepancy.</explanation>\n",
      "2025-06-04 01:13:33,356 - INFO - deepseek_v3_predict:197 - Row 2445 completed successfully in 2.63s - Prediction: A, Actual: A, Correct: True\n",
      "Processing questions with DeepSeek V3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:12<00:07,  3.73s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT : <prediction>A</prediction>  \n",
      "<explanation>Reviewing the steps taken during the investigations allows students to identify any differences in methodology, equipment, or execution that could have led to the differing results. This approach directly addresses the root cause of the discrepancy.</explanation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with DeepSeek V3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:13<00:03,  3.42s/question]2025-06-04 01:13:33,671 - INFO - deepseek_v3_predict:129 - Starting prediction for row 2432, question ID: LEAP_2004_8_10397\n",
      "2025-06-04 01:13:35,280 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-04 01:13:36,098 - INFO - make_api_call:83 - Row 2432 - Tokens used: 256 (prompt: 200, completion: 56)\n",
      "2025-06-04 01:13:36,103 - INFO - deepseek_v3_predict:160 - <prediction>B</prediction>  \n",
      "<explanation>The Moon has a weaker gravitational pull compared to Earth, which means that the force pulling people downward is less. This reduced gravity allows individuals to jump higher on the Moon than they can on Earth.</explanation>\n",
      "2025-06-04 01:13:36,103 - INFO - extract_prediction_and_explanation:42 - <prediction>B</prediction>  \n",
      "<explanation>The Moon has a weaker gravitational pull compared to Earth, which means that the force pulling people downward is less. This reduced gravity allows individuals to jump higher on the Moon than they can on Earth.</explanation>\n",
      "2025-06-04 01:13:36,103 - INFO - deepseek_v3_predict:197 - Row 2432 completed successfully in 2.43s - Prediction: B, Actual: B, Correct: True\n",
      "Processing questions with DeepSeek V3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:15<00:03,  3.42s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT : <prediction>B</prediction>  \n",
      "<explanation>The Moon has a weaker gravitational pull compared to Earth, which means that the force pulling people downward is less. This reduced gravity allows individuals to jump higher on the Moon than they can on Earth.</explanation>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with DeepSeek V3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:15<00:00,  3.16s/question]\n",
      "2025-06-04 01:13:36,537 - INFO - save_results:350 - Results saved to output\\20250604_011336_deepseek_v3_chunk_1_predictions.json\n",
      "2025-06-04 01:13:36,537 - INFO - save_results:368 - Summary saved to output\\20250604_011336_deepseek_v3_summary.csv\n",
      "2025-06-04 01:13:36,537 - INFO - process_dataframe:316 - Batch processing completed:\n",
      "2025-06-04 01:13:36,537 - INFO - process_dataframe:317 -   Total processed: 5\n",
      "2025-06-04 01:13:36,537 - INFO - process_dataframe:318 -   Successful predictions: 5\n",
      "2025-06-04 01:13:36,552 - INFO - process_dataframe:319 -   Failed predictions: 0\n",
      "2025-06-04 01:13:36,552 - INFO - process_dataframe:320 -   Correct answers: 5\n",
      "2025-06-04 01:13:36,552 - INFO - process_dataframe:321 -   Success rate: 100.00%\n",
      "2025-06-04 01:13:36,558 - INFO - process_dataframe:322 -   Accuracy rate: 100.00%\n",
      "2025-06-04 01:13:36,560 - INFO - process_dataframe:334 - Accuracy by split: {'validation': 100.0, 'test': 100.0}\n",
      "2025-06-04 01:13:36,563 - INFO - save_results:350 - Results saved to output\\20250604_011336_deepseek_v3_full_predictions_20250604_011336.json\n",
      "2025-06-04 01:13:36,568 - INFO - save_results:368 - Summary saved to output\\20250604_011336_deepseek_v3_summary.csv\n",
      "2025-06-04 01:13:36,580 - INFO - <module>:118 - Full dataset processing completed with DeepSeek V3 by alizahedzadeh. Success rate: 100.00%, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 1 completed:\n",
      "   üìà Success rate: 5/5 (100.0%)\n",
      "   üéØ Accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "üíæ Saving final combined results...\n",
      "\n",
      "============================================================\n",
      "üìä FINAL ANALYSIS - FULL DATASET WITH DEEPSEEK V3\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "DEEPSEEK V3 ANALYSIS RESULTS\n",
      "==================================================\n",
      "Total questions processed: 5\n",
      "Successful predictions: 5 (100.0%)\n",
      "Correct predictions: 5 (100.0% accuracy)\n",
      "\n",
      "Actual answer distribution:\n",
      "actual_answer\n",
      "A    2\n",
      "B    1\n",
      "C    1\n",
      "D    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Predicted answer distribution:\n",
      "prediction\n",
      "A    2\n",
      "B    1\n",
      "C    1\n",
      "D    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy by split:\n",
      "  VALIDATION: 3/3 (100.0%)\n",
      "  TEST: 2/2 (100.0%)\n",
      "\n",
      "‚è±Ô∏è  PERFORMANCE METRICS:\n",
      "   Total processing time: 14.2 seconds (0.2 minutes)\n",
      "   Average time per question: 2.84 seconds\n",
      "\n",
      "üí∞ COST ESTIMATION:\n",
      "   Total questions processed: 5\n",
      "   Model: DeepSeek V3 (Free tier on OpenRouter)\n",
      "   Estimated tokens per question: ~800-1200\n",
      "   Estimated total tokens: ~5,000\n",
      "\n",
      "üèÜ FINAL RESULTS SUMMARY:\n",
      "   üìä Total Questions: 5\n",
      "   ‚úÖ Successful Predictions: 5 (100.0%)\n",
      "   üéØ Overall Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your OpenRouter API key\n",
    "    # os.environ['OPENROUTER_API_KEY'] = 'your_openrouter_api_key_here'\n",
    "    \n",
    "    # Test API connection first\n",
    "    if not test_api_connection():\n",
    "        print(\"‚ùå API connection failed. Please check your OpenRouter API key and internet connection.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Load your data\n",
    "    base_df = cc.copy()\n",
    "    base_df.head()\n",
    "    base_df.dropna(inplace=True)\n",
    "    base_df.info()\n",
    "    base_df['answerKey'].value_counts()\n",
    "    mapping = {'1': 'A', '2': 'B', '3': 'C', '4': 'D'}\n",
    "    base_df['answerKey'] = base_df['answerKey'].replace(mapping)\n",
    "    base_df = base_df[base_df['answerKey'].isin(['A', 'B', 'C', 'D'])]\n",
    "    base_df.info()\n",
    "\n",
    "    # Configuration for full dataset processing\n",
    "    Config.REQUEST_DELAY = 0.3  # Conservative rate limiting for OpenRouter\n",
    "    Config.MAX_RETRIES = 5      # More retries for stability\n",
    "    Config.TIMEOUT = 45         # Longer timeout for complex questions\n",
    "\n",
    "    # Log the start of full processing\n",
    "    logger.info(f\"Starting full dataset processing with DeepSeek V3 by user: alizahedzadeh\")\n",
    "    logger.info(f\"Dataset size: {len(base_df)} questions\")\n",
    "    logger.info(f\"Processing start time (UTC): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Process the full dataset\n",
    "    print(\"üöÄ Starting full dataset processing with DeepSeek V3...\")\n",
    "    print(f\"üìä Total questions: {len(base_df)}\")\n",
    "    print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Process in manageable chunks to handle potential interruptions\n",
    "    chunk_size = 500\n",
    "    total_chunks = (len(base_df) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    print(f\"üì¶ Processing in {total_chunks} chunks of {chunk_size} questions each\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for chunk_num in range(total_chunks):\n",
    "        start_idx = chunk_num * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, len(base_df))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ Processing Chunk {chunk_num + 1}/{total_chunks} with DeepSeek V3\")\n",
    "        print(f\"üìç Rows {start_idx} to {end_idx-1} ({end_idx - start_idx} questions)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        chunk_results = process_dataframe(\n",
    "            base_df, \n",
    "            start_index=start_idx, \n",
    "            end_index=end_idx,\n",
    "            save_interval=50,  # Save every 50 questions\n",
    "            output_file=f\"deepseek_v3_chunk_{chunk_num + 1}_predictions.json\"\n",
    "        )\n",
    "        \n",
    "        all_results.extend(chunk_results)\n",
    "        \n",
    "        # Log chunk completion\n",
    "        chunk_successful = sum(1 for r in chunk_results if r['prediction'] is not None)\n",
    "        chunk_accuracy = sum(1 for r in chunk_results if r['is_correct'] is True) / chunk_successful * 100 if chunk_successful > 0 else 0\n",
    "        \n",
    "        print(f\"‚úÖ Chunk {chunk_num + 1} completed:\")\n",
    "        print(f\"   üìà Success rate: {chunk_successful}/{len(chunk_results)} ({chunk_successful/len(chunk_results)*100:.1f}%)\")\n",
    "        print(f\"   üéØ Accuracy: {chunk_accuracy:.1f}%\")\n",
    "        \n",
    "        # Small break between chunks\n",
    "        if chunk_num < total_chunks - 1:\n",
    "            print(\"‚è∏Ô∏è  Brief pause between chunks...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # Save final combined results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üíæ Saving final combined results...\")\n",
    "\n",
    "    final_output_file = f\"deepseek_v3_full_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    save_results(all_results, final_output_file)\n",
    "\n",
    "    # Final analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä FINAL ANALYSIS - FULL DATASET WITH DEEPSEEK V3\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    analyze_results(all_results)\n",
    "\n",
    "    # Additional detailed statistics\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    # Performance metrics\n",
    "    total_time = sum(r['processing_time'] for r in all_results)\n",
    "    avg_time_per_question = total_time / len(all_results)\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è  PERFORMANCE METRICS:\")\n",
    "    print(f\"   Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"   Average time per question: {avg_time_per_question:.2f} seconds\")\n",
    "\n",
    "    # Token usage summary (if available in logs)\n",
    "    print(f\"\\nüí∞ COST ESTIMATION:\")\n",
    "    print(f\"   Total questions processed: {len(all_results)}\")\n",
    "    print(f\"   Model: DeepSeek V3 (Free tier on OpenRouter)\")\n",
    "    print(f\"   Estimated tokens per question: ~800-1200\")\n",
    "    print(f\"   Estimated total tokens: ~{len(all_results) * 1000:,}\")\n",
    "\n",
    "    # Final success message\n",
    "    final_successful = sum(1 for r in all_results if r['prediction'] is not None)\n",
    "    final_accuracy = sum(1 for r in all_results if r['is_correct'] is True) / final_successful * 100 if final_successful > 0 else 0\n",
    "\n",
    "    print(f\"\\nüèÜ FINAL RESULTS SUMMARY:\")\n",
    "    print(f\"   üìä Total Questions: {len(all_results)}\")\n",
    "    print(f\"   ‚úÖ Successful Predictions: {final_successful} ({final_successful/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"   üéØ Overall Accuracy: {final_accuracy:.1f}%\")\n",
    "\n",
    "    logger.info(f\"Full dataset processing completed with DeepSeek V3 by alizahedzadeh. Success rate: {final_successful/len(all_results)*100:.2f}%, Accuracy: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30698471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('deepseek_v3_predict_c3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
