{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272c293a",
   "metadata": {},
   "source": [
    "# Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a582a336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchunk1.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk1.csv\", index=False)\\nchunk2.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk2.csv\", index=False)\\nchunk3.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk3.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../../../../datasets/question answering/ai2_arc/ARC-Challenge-clean(persian).csv\")  # Replace with actual file path\n",
    "\n",
    "# Split into 3 chunks\n",
    "chunk1 = df.iloc[:1000]\n",
    "chunk2 = df.iloc[1000:2000]\n",
    "chunk3 = df.iloc[2000:]\n",
    "\n",
    "# Optionally save each chunk\n",
    "'''\n",
    "chunk1.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk1.csv\", index=False)\n",
    "chunk2.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk2.csv\", index=False)\n",
    "chunk3.to_csv(\"../../../datasets/question answering/ai2_arc/ARC-Challenge-clean_chunk3.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "973bbc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2581 entries, 0 to 2580\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  2581 non-null   int64 \n",
      " 1   Unnamed: 0    2581 non-null   int64 \n",
      " 2   id            2581 non-null   object\n",
      " 3   question      2581 non-null   object\n",
      " 4   answerKey     2581 non-null   object\n",
      " 5   choice_A      2581 non-null   object\n",
      " 6   choice_B      2581 non-null   object\n",
      " 7   choice_C      2581 non-null   object\n",
      " 8   choice_D      2581 non-null   object\n",
      " 9   split         2581 non-null   object\n",
      " 10  question_fa   2581 non-null   object\n",
      " 11  choice_A_fa   2581 non-null   object\n",
      " 12  choice_B_fa   2581 non-null   object\n",
      " 13  choice_C_fa   2581 non-null   object\n",
      " 14  choice_D_fa   2581 non-null   object\n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 302.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../../datasets/question answering/ai2_arc/ARC-Challenge-clean(persian).csv\")  # Replace with actual file path\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2fa79d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 19:29:04,638 - INFO - setup_logging:31 - Logging initialized. Log file: logs\\20250605_192904_gpt_4o_mini_persian.log\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Any, Optional, List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging(log_file: str = \"gpt_4o_mini_persian.log\"):\n",
    "    \"\"\"Setup comprehensive logging configuration\"\"\"\n",
    "    log_dir = Path(\"logs\")\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    log_path = log_dir / f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{log_file}\"\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Logging initialized. Log file: {log_path}\")\n",
    "    return logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff54879",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdf2549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for the DeepSeek V3 prediction system\"\"\"\n",
    "    # OpenRouter API configuration\n",
    "    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', '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')\n",
    "    MODEL = \"openai/gpt-4o-mini\"  # DeepSeek V3 model on OpenRouter\n",
    "    TEMPERATURE = 0.0\n",
    "    MAX_RETRIES = 3\n",
    "    RETRY_DELAY = 1.0  # seconds\n",
    "    REQUEST_DELAY = 0.5  # delay between requests to avoid rate limiting\n",
    "    TIMEOUT = 30  # seconds\n",
    "    MAX_TOKENS = 1000\n",
    "\n",
    "# Initialize OpenRouter client (compatible with OpenAI client)\n",
    "client = OpenAI(\n",
    "    api_key=Config.OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19450642",
   "metadata": {},
   "source": [
    "# Updated prompt for questions without explicit passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fa05792",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_then_explain_prompt_context = '''\n",
    "You are given a multiple-choice question written in Persian.\n",
    "\n",
    "Step 1: Based on your knowledge and reasoning, select the most likely correct answer.  \n",
    "Step 2: Justify your answer with clear reasoning and explanation.\n",
    "\n",
    "Instructions:\n",
    "- Use logical reasoning to determine the best answer.\n",
    "- Do not reference the other answer options in your explanation.\n",
    "- Keep the explanation concise but informative (2-4 sentences).\n",
    "- Provide your explanation in Persian.\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "A) {option_A}  \n",
    "B) {option_B}  \n",
    "C) {option_C}  \n",
    "D) {option_D}\n",
    "\n",
    "Respond in this format:\n",
    "\n",
    "<prediction>A/B/C/D</prediction>  \n",
    "<explanation>[ÿ™Ÿàÿ∂€åÿ≠ Ÿà ÿßÿ≥ÿ™ÿØŸÑÿßŸÑ ÿ®Ÿá ÿ≤ÿ®ÿßŸÜ ŸÅÿßÿ±ÿ≥€å]</explanation>\n",
    "'''\n",
    "\n",
    "\n",
    "def validate_row(row: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate that a row has all required fields for your data structure\"\"\"\n",
    "    required_fields = ['question_fa', 'choice_A_fa', 'choice_B_fa', 'choice_C_fa', 'choice_D_fa']\n",
    "    missing_fields = [field for field in required_fields if field not in row or pd.isna(row[field]) or not str(row[field]).strip()]\n",
    "    \n",
    "    if missing_fields:\n",
    "        logger.warning(f\"Row missing required fields: {missing_fields}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_prediction_and_explanation(output: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Extract prediction and explanation from model output using regex\"\"\"\n",
    "    try:   \n",
    "        #logger.info(output)\n",
    "        # Extract prediction\n",
    "        pred_match = re.search(r\"<prediction>\\s*([A-D])\\s*</prediction>\", output, re.IGNORECASE)\n",
    "        prediction = pred_match.group(1).upper() if pred_match else None\n",
    "        \n",
    "        # Extract explanation\n",
    "        expl_match = re.search(r\"<explanation>\\s*(.*?)\\s*</explanation>\", output, re.DOTALL | re.IGNORECASE)\n",
    "        explanation = expl_match.group(1).strip() if expl_match else None\n",
    "        \n",
    "        if not prediction:\n",
    "            logger.warning(\"Could not extract prediction from output\")\n",
    "        if not explanation:\n",
    "            logger.warning(\"Could not extract explanation from output\")\n",
    "            \n",
    "        return {\n",
    "            \"prediction\": prediction,\n",
    "            \"explanation\": explanation\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting prediction/explanation: {e}\")\n",
    "        return {\"prediction\": None, \"explanation\": None}\n",
    "\n",
    "def make_api_call(prompt: str, row_id: Optional[int] = None) -> Optional[str]:\n",
    "    \"\"\"Make API call with retry logic and error handling using OpenRouter client\"\"\"\n",
    "    for attempt in range(Config.MAX_RETRIES):\n",
    "        try:\n",
    "            logger.debug(f\"Making API call (attempt {attempt + 1}/{Config.MAX_RETRIES}) for row {row_id}\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=Config.MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=Config.TEMPERATURE,\n",
    "                max_tokens=Config.MAX_TOKENS,\n",
    "                timeout=Config.TIMEOUT,\n",
    "            )\n",
    "            \n",
    "            output = response.choices[0].message.content\n",
    "            #print(\"MODEL OUTPUT :\", output)\n",
    "            # Log token usage\n",
    "            if hasattr(response, 'usage') and response.usage:\n",
    "                usage = response.usage\n",
    "                logger.info(f\"Row {row_id} - Tokens used: {usage.total_tokens} \"\n",
    "                           f\"(prompt: {usage.prompt_tokens}, \"\n",
    "                           f\"completion: {usage.completion_tokens})\")\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Handle different types of errors\n",
    "            if \"rate_limit\" in error_msg.lower() or \"429\" in error_msg:\n",
    "                wait_time = Config.RETRY_DELAY * (2 ** attempt)  # Exponential backoff\n",
    "                logger.warning(f\"Rate limit hit for row {row_id}, waiting {wait_time}s before retry {attempt + 1}\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "            elif \"timeout\" in error_msg.lower():\n",
    "                logger.error(f\"API timeout for row {row_id}: {error_msg}\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(Config.RETRY_DELAY)\n",
    "                    \n",
    "            elif \"api\" in error_msg.lower():\n",
    "                logger.error(f\"OpenRouter API error for row {row_id}: {error_msg}\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(Config.RETRY_DELAY)\n",
    "                    \n",
    "            else:\n",
    "                logger.error(f\"Unexpected error ({error_type}) for row {row_id}: {error_msg}\")\n",
    "                if attempt < Config.MAX_RETRIES - 1:\n",
    "                    time.sleep(Config.RETRY_DELAY)\n",
    "    \n",
    "    logger.error(f\"Failed to get response for row {row_id} after {Config.MAX_RETRIES} attempts\")\n",
    "    return None\n",
    "\n",
    "def predict(row: Dict[str, Any], row_id: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced prediction function adapted for your data structure\n",
    "    \n",
    "    Args:\n",
    "        row: Dictionary containing question data with columns: question, choice_A, choice_B, choice_C, choice_D\n",
    "        row_id: Optional row identifier for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting prediction for row {row_id}, question ID: {row.get('id', 'N/A')}\")\n",
    "    \n",
    "    # Validate input\n",
    "    if not validate_row(row):\n",
    "        logger.error(f\"Row {row_id} validation failed\")\n",
    "        return {\n",
    "            \"row_id\": row_id,\n",
    "            \"question_id\": row.get('id'),\n",
    "            \"prediction\": None,\n",
    "            \"explanation\": None,\n",
    "            \"raw_output\": None,\n",
    "            \"actual_answer\": row.get('answerKey'),\n",
    "            \"is_correct\": None,\n",
    "            \"error\": \"Invalid input data\",\n",
    "            \"processing_time\": 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Format prompt using your column names\n",
    "        prompt = predict_then_explain_prompt_context.format(\n",
    "            question=row['question'],\n",
    "            option_A=row['choice_A'],\n",
    "            option_B=row['choice_B'],\n",
    "            option_C=row['choice_C'],\n",
    "            option_D=row['choice_D'],\n",
    "        )\n",
    "        \n",
    "        logger.debug(f\"Row {row_id} - Prompt length: {len(prompt)} characters\")\n",
    "        \n",
    "        # Make API call\n",
    "        output = make_api_call(prompt, row_id)\n",
    "        #logger.info(output)\n",
    "        if output is None:\n",
    "            return {\n",
    "                \"row_id\": row_id,\n",
    "                \"question_id\": row.get('id'),\n",
    "                \"prediction\": None,\n",
    "                \"explanation\": None,\n",
    "                \"raw_output\": None,\n",
    "                \"actual_answer\": row.get('answerKey'),\n",
    "                \"is_correct\": None,\n",
    "                \"error\": \"API call failed\",\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # Extract prediction and explanation\n",
    "        extracted = extract_prediction_and_explanation(output)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        actual_answer = row.get('answerKey')\n",
    "        predicted_answer = extracted[\"prediction\"]\n",
    "        is_correct = predicted_answer == actual_answer if (predicted_answer and actual_answer) else None\n",
    "        \n",
    "        result = {\n",
    "            \"row_id\": row_id,\n",
    "            \"question_id\": row.get('id'),\n",
    "            \"prediction\": predicted_answer,\n",
    "            \"explanation\": extracted[\"explanation\"],\n",
    "            \"raw_output\": output,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"split\": row.get('split'),\n",
    "            \"error\": None,\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Row {row_id} completed successfully in {processing_time:.2f}s - \"\n",
    "                   f\"Prediction: {result['prediction']}, Actual: {actual_answer}, \"\n",
    "                   f\"Correct: {is_correct}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.error(f\"Unexpected error processing row {row_id}: {e}\")\n",
    "        return {\n",
    "            \"row_id\": row_id,\n",
    "            \"question_id\": row.get('id'),\n",
    "            \"prediction\": None,\n",
    "            \"explanation\": None,\n",
    "            \"raw_output\": output,\n",
    "            \"actual_answer\": row.get('answerKey'),\n",
    "            \"is_correct\": None,\n",
    "            \"error\": str(e),\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, \n",
    "                     save_interval: int = 50,\n",
    "                     output_file: str = \"_predictions_results.json\",\n",
    "                     start_index: int = 0,\n",
    "                     end_index: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a pandas DataFrame with your specific column structure\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns: question, choice_A, choice_B, choice_C, choice_D, answerKey, etc.\n",
    "        save_interval: Save results every N processed items\n",
    "        output_file: File to save results to\n",
    "        start_index: Start processing from this index (useful for resuming)\n",
    "        end_index: Stop processing at this index (useful for testing subsets)\n",
    "        \n",
    "    Returns:\n",
    "        List of prediction results\n",
    "    \"\"\"\n",
    "    if end_index is None:\n",
    "        end_index = len(df)\n",
    "    \n",
    "    subset_df = df.iloc[start_index:end_index]\n",
    "    \n",
    "    logger.info(f\"Starting batch processing of {len(subset_df)} items (rows {start_index} to {end_index-1})\")\n",
    "    logger.info(f\"DataFrame info: {len(df)} total rows, columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Log data distribution\n",
    "    if 'split' in df.columns:\n",
    "        split_counts = df['split'].value_counts()\n",
    "        logger.info(f\"Data split distribution: {split_counts.to_dict()}\")\n",
    "    \n",
    "    if 'answerKey' in df.columns:\n",
    "        answer_dist = df['answerKey'].value_counts()\n",
    "        logger.info(f\"Answer distribution: {answer_dist.to_dict()}\")\n",
    "    \n",
    "    results = []\n",
    "    failed_count = 0\n",
    "    correct_count = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(\n",
    "        subset_df.iterrows(), \n",
    "        total=len(subset_df),\n",
    "        desc=\"Processing questions with GPT 4o mini\",\n",
    "        unit=\"question\",\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for original_idx, row in pbar:\n",
    "            # Process the row\n",
    "            result = predict(row.to_dict(), row_id=original_idx)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update counters\n",
    "            if result['prediction'] is None:\n",
    "                failed_count += 1\n",
    "            elif result['is_correct'] is True:\n",
    "                correct_count += 1\n",
    "            \n",
    "            # Update progress bar description\n",
    "            total_processed = len(results)\n",
    "            success_rate = ((total_processed - failed_count) / total_processed * 100) if total_processed > 0 else 0\n",
    "            accuracy = (correct_count / (total_processed - failed_count) * 100) if (total_processed - failed_count) > 0 else 0\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Success': f'{success_rate:.1f}%',\n",
    "                'Accuracy': f'{accuracy:.1f}%',\n",
    "                'Last': result['prediction'] or 'FAIL'\n",
    "            })\n",
    "            \n",
    "            # Periodic saving\n",
    "            if len(results) % save_interval == 0:\n",
    "                save_results(results, output_file)\n",
    "                logger.info(f\"Intermediate save completed at {len(results)} processed items\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(Config.REQUEST_DELAY)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Processing interrupted by user\")\n",
    "        pbar.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during batch processing: {e}\")\n",
    "        pbar.close()\n",
    "        \n",
    "    finally:\n",
    "        # Final save\n",
    "        save_results(results, output_file)\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_processed = len(results)\n",
    "        successful = sum(1 for r in results if r['prediction'] is not None)\n",
    "        correct = sum(1 for r in results if r['is_correct'] is True)\n",
    "        success_rate = successful / total_processed * 100 if total_processed > 0 else 0\n",
    "        accuracy = correct / successful * 100 if successful > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Batch processing completed:\")\n",
    "        logger.info(f\"  Total processed: {total_processed}\")\n",
    "        logger.info(f\"  Successful predictions: {successful}\")\n",
    "        logger.info(f\"  Failed predictions: {total_processed - successful}\")\n",
    "        logger.info(f\"  Correct answers: {correct}\")\n",
    "        logger.info(f\"  Success rate: {success_rate:.2f}%\")\n",
    "        logger.info(f\"  Accuracy rate: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Split-wise accuracy if available\n",
    "        if successful > 0:\n",
    "            splits_accuracy = {}\n",
    "            for split_name in df['split'].unique() if 'split' in df.columns else ['all']:\n",
    "                split_results = [r for r in results if r.get('split') == split_name or split_name == 'all']\n",
    "                split_correct = sum(1 for r in split_results if r['is_correct'] is True)\n",
    "                split_successful = sum(1 for r in split_results if r['prediction'] is not None)\n",
    "                if split_successful > 0:\n",
    "                    splits_accuracy[split_name] = split_correct / split_successful * 100\n",
    "            \n",
    "            logger.info(f\"Accuracy by split: {splits_accuracy}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str):\n",
    "    \"\"\"Save results to JSON file with error handling\"\"\"\n",
    "    try:\n",
    "        output_path = Path(\"output\") \n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        full_path = output_path / f\"{timestamp}_{filename}\"\n",
    "        \n",
    "        with open(full_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        logger.info(f\"Results saved to {full_path}\")\n",
    "        \n",
    "        # Also save a summary CSV for easy analysis\n",
    "        summary_path = output_path / f\"{timestamp}_deepseek_v3_summary.csv\"\n",
    "        summary_df = pd.DataFrame([\n",
    "            {\n",
    "                'row_id': r['row_id'],\n",
    "                'question_id': r['question_id'],\n",
    "                'prediction': r['prediction'],\n",
    "                'actual_answer': r['actual_answer'],\n",
    "                'is_correct': r['is_correct'],\n",
    "                'split': r.get('split'),\n",
    "                'has_error': r['error'] is not None,\n",
    "                'processing_time': r['processing_time']\n",
    "            }\n",
    "            for r in results\n",
    "        ])\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        logger.info(f\"Summary saved to {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "\n",
    "def analyze_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Analyze and print results statistics\"\"\"\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GPT 4O mini ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total = len(df_results)\n",
    "    successful = len(df_results[df_results['prediction'].notna()])\n",
    "    correct = len(df_results[df_results['is_correct'] == True])\n",
    "    \n",
    "    print(f\"Total questions processed: {total}\")\n",
    "    print(f\"Successful predictions: {successful} ({successful/total*100:.1f}%)\")\n",
    "    if successful > 0:\n",
    "        print(f\"Correct predictions: {correct} ({correct/successful*100:.1f}% accuracy)\")\n",
    "    else:\n",
    "        print(\"Correct predictions: 0 (No successful predictions)\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = df_results[df_results['error'].notna()]\n",
    "    if len(errors) > 0:\n",
    "        print(f\"\\nErrors encountered: {len(errors)}\")\n",
    "        error_types = errors['error'].value_counts()\n",
    "        for error_type, count in error_types.items():\n",
    "            print(f\"  {error_type}: {count}\")\n",
    "    \n",
    "    # Answer distribution\n",
    "    if 'actual_answer' in df_results.columns:\n",
    "        print(f\"\\nActual answer distribution:\")\n",
    "        print(df_results['actual_answer'].value_counts().sort_index())\n",
    "    \n",
    "    if successful > 0 and 'prediction' in df_results.columns:\n",
    "        print(f\"\\nPredicted answer distribution:\")\n",
    "        pred_dist = df_results[df_results['prediction'].notna()]['prediction'].value_counts().sort_index()\n",
    "        print(pred_dist)\n",
    "    \n",
    "    # Split analysis if available\n",
    "    if 'split' in df_results.columns and successful > 0:\n",
    "        print(f\"\\nAccuracy by split:\")\n",
    "        for split in df_results['split'].unique():\n",
    "            if pd.notna(split):\n",
    "                split_df = df_results[df_results['split'] == split]\n",
    "                split_correct = len(split_df[split_df['is_correct'] == True])\n",
    "                split_total = len(split_df[split_df['prediction'].notna()])\n",
    "                if split_total > 0:\n",
    "                    accuracy = split_correct / split_total * 100\n",
    "                    print(f\"  {split.upper()}: {split_correct}/{split_total} ({accuracy:.1f}%)\")\n",
    "\n",
    "# Test API connection\n",
    "def test_api_connection():\n",
    "    \"\"\"Test if the OpenRouter API is working properly with Gpt 4o mini\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Testing OpenRouter Gpt 4o mini API connection...\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=Config.MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello, please respond with 'DeepSeek V3 API working'.\"}],\n",
    "            max_tokens=10,\n",
    "            temperature=0,\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": \"https://github.com/alizahedzadeh\",\n",
    "                \"X-Title\": \"Gpt 4o mini API Test\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        logger.info(f\"API test successful. Response: {result}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"API test failed: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94f8db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = chunk3.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6afdd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 19:29:04,782 - INFO - test_api_connection:428 - Testing OpenRouter Gpt 4o mini API connection...\n",
      "2025-06-05 19:29:05,575 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:05,704 - INFO - test_api_connection:442 - API test successful. Response: DeepSeek V3 API working.\n",
      "2025-06-05 19:29:05,726 - INFO - <module>:28 - Starting full dataset processing with DeepSeek V3 by user: alizahedzadeh\n",
      "2025-06-05 19:29:05,727 - INFO - <module>:29 - Dataset size: 2581 questions\n",
      "2025-06-05 19:29:05,727 - INFO - <module>:30 - Processing start time (UTC): 2025-06-05 19:29:05\n",
      "2025-06-05 19:29:05,728 - INFO - process_dataframe:242 - Starting batch processing of 500 items (rows 0 to 499)\n",
      "2025-06-05 19:29:05,728 - INFO - process_dataframe:243 - DataFrame info: 2581 total rows, columns: ['Unnamed: 0.1', 'Unnamed: 0', 'id', 'question', 'answerKey', 'choice_A', 'choice_B', 'choice_C', 'choice_D', 'split', 'question_fa', 'choice_A_fa', 'choice_B_fa', 'choice_C_fa', 'choice_D_fa']\n",
      "2025-06-05 19:29:05,730 - INFO - process_dataframe:248 - Data split distribution: {'test': 1168, 'train': 1118, 'validation': 295}\n",
      "2025-06-05 19:29:05,732 - INFO - process_dataframe:252 - Answer distribution: {'C': 678, 'B': 677, 'D': 661, 'A': 565}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2581 entries, 0 to 2580\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  2581 non-null   int64 \n",
      " 1   Unnamed: 0    2581 non-null   int64 \n",
      " 2   id            2581 non-null   object\n",
      " 3   question      2581 non-null   object\n",
      " 4   answerKey     2581 non-null   object\n",
      " 5   choice_A      2581 non-null   object\n",
      " 6   choice_B      2581 non-null   object\n",
      " 7   choice_C      2581 non-null   object\n",
      " 8   choice_D      2581 non-null   object\n",
      " 9   split         2581 non-null   object\n",
      " 10  question_fa   2581 non-null   object\n",
      " 11  choice_A_fa   2581 non-null   object\n",
      " 12  choice_B_fa   2581 non-null   object\n",
      " 13  choice_C_fa   2581 non-null   object\n",
      " 14  choice_D_fa   2581 non-null   object\n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 302.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2581 entries, 0 to 2580\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  2581 non-null   int64 \n",
      " 1   Unnamed: 0    2581 non-null   int64 \n",
      " 2   id            2581 non-null   object\n",
      " 3   question      2581 non-null   object\n",
      " 4   answerKey     2581 non-null   object\n",
      " 5   choice_A      2581 non-null   object\n",
      " 6   choice_B      2581 non-null   object\n",
      " 7   choice_C      2581 non-null   object\n",
      " 8   choice_D      2581 non-null   object\n",
      " 9   split         2581 non-null   object\n",
      " 10  question_fa   2581 non-null   object\n",
      " 11  choice_A_fa   2581 non-null   object\n",
      " 12  choice_B_fa   2581 non-null   object\n",
      " 13  choice_C_fa   2581 non-null   object\n",
      " 14  choice_D_fa   2581 non-null   object\n",
      "dtypes: int64(2), object(13)\n",
      "memory usage: 302.6+ KB\n",
      "üöÄ Starting full dataset processing with DeepSeek V3...\n",
      "üìä Total questions: 2581\n",
      "‚è∞ Start time: 2025-06-05 19:29:05\n",
      "üì¶ Processing in 6 chunks of 500 questions each\n",
      "\n",
      "============================================================\n",
      "üîÑ Processing Chunk 1/6 with DeepSeek V3\n",
      "üìç Rows 0 to 499 (500 questions)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with GPT 4o mini:   0%|          | 0/500 [00:00<?, ?question/s]2025-06-05 19:29:05,734 - INFO - predict:130 - Starting prediction for row 0, question ID: Mercury_SC_415702\n",
      "2025-06-05 19:29:06,372 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:07,311 - INFO - make_api_call:84 - Row 0 - Tokens used: 257 (prompt: 179, completion: 78)\n",
      "2025-06-05 19:29:07,311 - INFO - predict:198 - Row 0 completed successfully in 1.58s - Prediction: A, Actual: A, Correct: True\n",
      "Processing questions with GPT 4o mini:   0%|          | 1/500 [00:01<15:38,  1.88s/question]2025-06-05 19:29:07,615 - INFO - predict:130 - Starting prediction for row 1, question ID: MCAS_2009_5_6516\n",
      "2025-06-05 19:29:08,099 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:09,462 - INFO - make_api_call:84 - Row 1 - Tokens used: 277 (prompt: 191, completion: 86)\n",
      "2025-06-05 19:29:09,463 - INFO - predict:198 - Row 1 completed successfully in 1.85s - Prediction: B, Actual: B, Correct: True\n",
      "Processing questions with GPT 4o mini:   0%|          | 2/500 [00:04<16:59,  2.05s/question]2025-06-05 19:29:09,783 - INFO - predict:130 - Starting prediction for row 2, question ID: Mercury_7233695\n",
      "2025-06-05 19:29:10,255 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Processing questions with GPT 4o mini:   0%|          | 2/500 [00:06<26:25,  3.18s/question]\n",
      "2025-06-05 19:29:12,103 - WARNING - process_dataframe:299 - Processing interrupted by user\n",
      "2025-06-05 19:29:12,208 - INFO - save_results:351 - Results saved to output\\20250605_192912_deepseek_v3_chunk_1_predictions.json\n",
      "2025-06-05 19:29:12,211 - INFO - save_results:369 - Summary saved to output\\20250605_192912_deepseek_v3_summary.csv\n",
      "2025-06-05 19:29:12,212 - INFO - process_dataframe:317 - Batch processing completed:\n",
      "2025-06-05 19:29:12,212 - INFO - process_dataframe:318 -   Total processed: 2\n",
      "2025-06-05 19:29:12,213 - INFO - process_dataframe:319 -   Successful predictions: 2\n",
      "2025-06-05 19:29:12,213 - INFO - process_dataframe:320 -   Failed predictions: 0\n",
      "2025-06-05 19:29:12,214 - INFO - process_dataframe:321 -   Correct answers: 2\n",
      "2025-06-05 19:29:12,215 - INFO - process_dataframe:322 -   Success rate: 100.00%\n",
      "2025-06-05 19:29:12,216 - INFO - process_dataframe:323 -   Accuracy rate: 100.00%\n",
      "2025-06-05 19:29:12,217 - INFO - process_dataframe:335 - Accuracy by split: {'train': 100.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 1 completed:\n",
      "   üìà Success rate: 2/2 (100.0%)\n",
      "   üéØ Accuracy: 100.0%\n",
      "‚è∏Ô∏è  Brief pause between chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 19:29:14,224 - INFO - process_dataframe:242 - Starting batch processing of 500 items (rows 500 to 999)\n",
      "2025-06-05 19:29:14,225 - INFO - process_dataframe:243 - DataFrame info: 2581 total rows, columns: ['Unnamed: 0.1', 'Unnamed: 0', 'id', 'question', 'answerKey', 'choice_A', 'choice_B', 'choice_C', 'choice_D', 'split', 'question_fa', 'choice_A_fa', 'choice_B_fa', 'choice_C_fa', 'choice_D_fa']\n",
      "2025-06-05 19:29:14,226 - INFO - process_dataframe:248 - Data split distribution: {'test': 1168, 'train': 1118, 'validation': 295}\n",
      "2025-06-05 19:29:14,228 - INFO - process_dataframe:252 - Answer distribution: {'C': 678, 'B': 677, 'D': 661, 'A': 565}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ Processing Chunk 2/6 with DeepSeek V3\n",
      "üìç Rows 500 to 999 (500 questions)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions with GPT 4o mini:   0%|          | 0/500 [00:00<?, ?question/s]2025-06-05 19:29:14,230 - INFO - predict:130 - Starting prediction for row 500, question ID: MDSA_2011_8_1\n",
      "2025-06-05 19:29:15,618 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:17,479 - INFO - make_api_call:84 - Row 500 - Tokens used: 334 (prompt: 222, completion: 112)\n",
      "2025-06-05 19:29:17,481 - INFO - predict:198 - Row 500 completed successfully in 3.25s - Prediction: D, Actual: D, Correct: True\n",
      "Processing questions with GPT 4o mini:   0%|          | 1/500 [00:03<29:40,  3.57s/question]2025-06-05 19:29:17,797 - INFO - predict:130 - Starting prediction for row 501, question ID: MDSA_2009_5_17\n",
      "2025-06-05 19:29:18,465 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:20,020 - INFO - make_api_call:84 - Row 501 - Tokens used: 292 (prompt: 198, completion: 94)\n",
      "2025-06-05 19:29:20,020 - INFO - predict:198 - Row 501 completed successfully in 2.22s - Prediction: C, Actual: C, Correct: True\n",
      "Processing questions with GPT 4o mini:   0%|          | 2/500 [00:06<24:33,  2.96s/question]2025-06-05 19:29:20,331 - INFO - predict:130 - Starting prediction for row 502, question ID: Mercury_SC_LBS10791\n",
      "2025-06-05 19:29:20,856 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:22,510 - INFO - make_api_call:84 - Row 502 - Tokens used: 293 (prompt: 194, completion: 99)\n",
      "2025-06-05 19:29:22,512 - INFO - predict:198 - Row 502 completed successfully in 2.18s - Prediction: C, Actual: C, Correct: True\n",
      "Processing questions with GPT 4o mini:   1%|          | 3/500 [00:08<22:43,  2.74s/question]2025-06-05 19:29:22,816 - INFO - predict:130 - Starting prediction for row 503, question ID: Mercury_7027055\n",
      "2025-06-05 19:29:23,354 - INFO - _send_single_request:1025 - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 19:29:24,737 - INFO - make_api_call:84 - Row 503 - Tokens used: 293 (prompt: 190, completion: 103)\n",
      "2025-06-05 19:29:24,739 - INFO - predict:198 - Row 503 completed successfully in 1.92s - Prediction: C, Actual: C, Correct: True\n",
      "Processing questions with GPT 4o mini:   1%|          | 3/500 [00:10<22:43,  2.74s/question]"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your OpenRouter API key\n",
    "    # os.environ['OPENROUTER_API_KEY'] = 'your_openrouter_api_key_here'\n",
    "    \n",
    "    # Test API connection first\n",
    "    if not test_api_connection():\n",
    "        print(\"‚ùå API connection failed. Please check your OpenRouter API key and internet connection.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Load your data\n",
    "    base_df = df.copy()\n",
    "    base_df.head()\n",
    "    base_df.dropna(inplace=True)\n",
    "    base_df.info()\n",
    "    base_df['answerKey'].value_counts()\n",
    "    mapping = {'1': 'A', '2': 'B', '3': 'C', '4': 'D'}\n",
    "    base_df['answerKey'] = base_df['answerKey'].replace(mapping)\n",
    "    base_df = base_df[base_df['answerKey'].isin(['A', 'B', 'C', 'D'])]\n",
    "    base_df.info()\n",
    "\n",
    "    # Configuration for full dataset processing\n",
    "    Config.REQUEST_DELAY = 0.3  # Conservative rate limiting for OpenRouter\n",
    "    Config.MAX_RETRIES = 5      # More retries for stability\n",
    "    Config.TIMEOUT = 45         # Longer timeout for complex questions\n",
    "\n",
    "    # Log the start of full processing\n",
    "    logger.info(f\"Starting full dataset processing with DeepSeek V3 by user: alizahedzadeh\")\n",
    "    logger.info(f\"Dataset size: {len(base_df)} questions\")\n",
    "    logger.info(f\"Processing start time (UTC): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Process the full dataset\n",
    "    print(\"üöÄ Starting full dataset processing with DeepSeek V3...\")\n",
    "    print(f\"üìä Total questions: {len(base_df)}\")\n",
    "    print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Process in manageable chunks to handle potential interruptions\n",
    "    chunk_size = 500\n",
    "    total_chunks = (len(base_df) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    print(f\"üì¶ Processing in {total_chunks} chunks of {chunk_size} questions each\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for chunk_num in range(total_chunks):\n",
    "        start_idx = chunk_num * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, len(base_df))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ Processing Chunk {chunk_num + 1}/{total_chunks} with DeepSeek V3\")\n",
    "        print(f\"üìç Rows {start_idx} to {end_idx-1} ({end_idx - start_idx} questions)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        chunk_results = process_dataframe(\n",
    "            base_df, \n",
    "            start_index=start_idx, \n",
    "            end_index=end_idx,\n",
    "            save_interval=50,  # Save every 50 questions\n",
    "            output_file=f\"deepseek_v3_chunk_{chunk_num + 1}_predictions.json\"\n",
    "        )\n",
    "        \n",
    "        all_results.extend(chunk_results)\n",
    "        \n",
    "        # Log chunk completion\n",
    "        chunk_successful = sum(1 for r in chunk_results if r['prediction'] is not None)\n",
    "        chunk_accuracy = sum(1 for r in chunk_results if r['is_correct'] is True) / chunk_successful * 100 if chunk_successful > 0 else 0\n",
    "        \n",
    "        print(f\"‚úÖ Chunk {chunk_num + 1} completed:\")\n",
    "        print(f\"   üìà Success rate: {chunk_successful}/{len(chunk_results)} ({chunk_successful/len(chunk_results)*100:.1f}%)\")\n",
    "        print(f\"   üéØ Accuracy: {chunk_accuracy:.1f}%\")\n",
    "        \n",
    "        # Small break between chunks\n",
    "        if chunk_num < total_chunks - 1:\n",
    "            print(\"‚è∏Ô∏è  Brief pause between chunks...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # Save final combined results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üíæ Saving final combined results...\")\n",
    "\n",
    "    final_output_file = f\"gpt_4o_mini_full_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    save_results(all_results, final_output_file)\n",
    "\n",
    "    # Final analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä FINAL ANALYSIS - FULL DATASET WITH Gpt 4o mini\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    analyze_results(all_results)\n",
    "\n",
    "    # Additional detailed statistics\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    # Performance metrics\n",
    "    total_time = sum(r['processing_time'] for r in all_results)\n",
    "    avg_time_per_question = total_time / len(all_results)\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è  PERFORMANCE METRICS:\")\n",
    "    print(f\"   Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"   Average time per question: {avg_time_per_question:.2f} seconds\")\n",
    "\n",
    "    # Token usage summary (if available in logs)\n",
    "    print(f\"\\nüí∞ COST ESTIMATION:\")\n",
    "    print(f\"   Total questions processed: {len(all_results)}\")\n",
    "    print(f\"   Model: Gpt 4o mini\")\n",
    "    print(f\"   Estimated tokens per question: ~800-1200\")\n",
    "    print(f\"   Estimated total tokens: ~{len(all_results) * 1000:,}\")\n",
    "\n",
    "    # Final success message\n",
    "    final_successful = sum(1 for r in all_results if r['prediction'] is not None)\n",
    "    final_accuracy = sum(1 for r in all_results if r['is_correct'] is True) / final_successful * 100 if final_successful > 0 else 0\n",
    "\n",
    "    print(f\"\\nüèÜ FINAL RESULTS SUMMARY:\")\n",
    "    print(f\"   üìä Total Questions: {len(all_results)}\")\n",
    "    print(f\"   ‚úÖ Successful Predictions: {final_successful} ({final_successful/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"   üéØ Overall Accuracy: {final_accuracy:.1f}%\")\n",
    "\n",
    "    logger.info(f\"Full dataset processing completed with Gpt 4o mini by alizahedzadeh. Success rate: {final_successful/len(all_results)*100:.2f}%, Accuracy: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed7d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>explanation</th>\n",
       "      <th>raw_output</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>split</th>\n",
       "      <th>error</th>\n",
       "      <th>processing_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2576</td>\n",
       "      <td>Mercury_7090598</td>\n",
       "      <td>C</td>\n",
       "      <td>ŸÅÿ±ÿß€åŸÜÿØ ÿ≤€åÿ±ÿ±ŸÅÿ™ (subduction) ÿ¥ÿßŸÖŸÑ ÿßŸÜÿ™ŸÇÿßŸÑ ŸÖŸÇÿØÿßÿ± ÿ≤...</td>\n",
       "      <td>&lt;prediction&gt;C&lt;/prediction&gt;  \\n&lt;explanation&gt;ŸÅÿ±ÿß...</td>\n",
       "      <td>C</td>\n",
       "      <td>True</td>\n",
       "      <td>validation</td>\n",
       "      <td>None</td>\n",
       "      <td>3.588926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2278</td>\n",
       "      <td>Mercury_7108990</td>\n",
       "      <td>A</td>\n",
       "      <td>⁄©Ÿá⁄©ÿ¥ÿßŸÜ‚ÄåŸáÿß ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿ≥ÿßÿÆÿ™ÿßÿ±Ÿáÿß€å ÿ®ÿ≤ÿ±⁄Ø Ÿà Ÿàÿ≥€åÿπ ÿØÿ± ÿ¨Ÿá...</td>\n",
       "      <td>&lt;prediction&gt;A&lt;/prediction&gt;  \\n&lt;explanation&gt;⁄©Ÿá⁄©...</td>\n",
       "      <td>A</td>\n",
       "      <td>True</td>\n",
       "      <td>test</td>\n",
       "      <td>None</td>\n",
       "      <td>2.793035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2104</td>\n",
       "      <td>Mercury_184818</td>\n",
       "      <td>D</td>\n",
       "      <td>ÿØÿ± ŸÖÿ≠€åÿ∑€å ÿ®ÿß ŸÖ€åÿØÿßŸÜ ⁄Øÿ±ÿßŸÜÿ¥€å ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≤ ÿ≤ŸÖ€åŸÜÿå ŸÖŸàÿ¨ŸàÿØÿß...</td>\n",
       "      <td>&lt;prediction&gt;D&lt;/prediction&gt;  \\n&lt;explanation&gt;ÿØÿ± ...</td>\n",
       "      <td>D</td>\n",
       "      <td>True</td>\n",
       "      <td>test</td>\n",
       "      <td>None</td>\n",
       "      <td>2.853761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2445</td>\n",
       "      <td>Mercury_SC_407314</td>\n",
       "      <td>A</td>\n",
       "      <td>ÿ®Ÿáÿ™ÿ±€åŸÜ ÿ±ÿßŸá ÿ®ÿ±ÿß€å ÿØÿßŸÜÿ¥‚Äåÿ¢ŸÖŸàÿ≤ÿßŸÜ ÿ®ÿ±ÿß€å ÿ™ÿπ€å€åŸÜ ÿØŸÑ€åŸÑ ÿ™ŸÅ...</td>\n",
       "      <td>&lt;prediction&gt;A&lt;/prediction&gt;  \\n&lt;explanation&gt;ÿ®Ÿáÿ™...</td>\n",
       "      <td>A</td>\n",
       "      <td>True</td>\n",
       "      <td>validation</td>\n",
       "      <td>None</td>\n",
       "      <td>2.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2432</td>\n",
       "      <td>LEAP_2004_8_10397</td>\n",
       "      <td>B</td>\n",
       "      <td>ŸÖÿßŸá ŸÜ€åÿ±Ÿà€å ⁄Øÿ±ÿßŸÜÿ¥€å ⁄©ŸÖÿ™ÿ±€å ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ÿ≤ŸÖ€åŸÜ ÿØÿßÿ±ÿØÿå ÿ®Ÿá Ÿá...</td>\n",
       "      <td>&lt;prediction&gt;B&lt;/prediction&gt;  \\n&lt;explanation&gt;ŸÖÿßŸá...</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "      <td>validation</td>\n",
       "      <td>None</td>\n",
       "      <td>2.406545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id        question_id prediction  \\\n",
       "0    2576    Mercury_7090598          C   \n",
       "1    2278    Mercury_7108990          A   \n",
       "2    2104     Mercury_184818          D   \n",
       "3    2445  Mercury_SC_407314          A   \n",
       "4    2432  LEAP_2004_8_10397          B   \n",
       "\n",
       "                                         explanation  \\\n",
       "0  ŸÅÿ±ÿß€åŸÜÿØ ÿ≤€åÿ±ÿ±ŸÅÿ™ (subduction) ÿ¥ÿßŸÖŸÑ ÿßŸÜÿ™ŸÇÿßŸÑ ŸÖŸÇÿØÿßÿ± ÿ≤...   \n",
       "1  ⁄©Ÿá⁄©ÿ¥ÿßŸÜ‚ÄåŸáÿß ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿ≥ÿßÿÆÿ™ÿßÿ±Ÿáÿß€å ÿ®ÿ≤ÿ±⁄Ø Ÿà Ÿàÿ≥€åÿπ ÿØÿ± ÿ¨Ÿá...   \n",
       "2  ÿØÿ± ŸÖÿ≠€åÿ∑€å ÿ®ÿß ŸÖ€åÿØÿßŸÜ ⁄Øÿ±ÿßŸÜÿ¥€å ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≤ ÿ≤ŸÖ€åŸÜÿå ŸÖŸàÿ¨ŸàÿØÿß...   \n",
       "3  ÿ®Ÿáÿ™ÿ±€åŸÜ ÿ±ÿßŸá ÿ®ÿ±ÿß€å ÿØÿßŸÜÿ¥‚Äåÿ¢ŸÖŸàÿ≤ÿßŸÜ ÿ®ÿ±ÿß€å ÿ™ÿπ€å€åŸÜ ÿØŸÑ€åŸÑ ÿ™ŸÅ...   \n",
       "4  ŸÖÿßŸá ŸÜ€åÿ±Ÿà€å ⁄Øÿ±ÿßŸÜÿ¥€å ⁄©ŸÖÿ™ÿ±€å ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ÿ≤ŸÖ€åŸÜ ÿØÿßÿ±ÿØÿå ÿ®Ÿá Ÿá...   \n",
       "\n",
       "                                          raw_output actual_answer  \\\n",
       "0  <prediction>C</prediction>  \\n<explanation>ŸÅÿ±ÿß...             C   \n",
       "1  <prediction>A</prediction>  \\n<explanation>⁄©Ÿá⁄©...             A   \n",
       "2  <prediction>D</prediction>  \\n<explanation>ÿØÿ± ...             D   \n",
       "3  <prediction>A</prediction>  \\n<explanation>ÿ®Ÿáÿ™...             A   \n",
       "4  <prediction>B</prediction>  \\n<explanation>ŸÖÿßŸá...             B   \n",
       "\n",
       "   is_correct       split error  processing_time  \n",
       "0        True  validation  None         3.588926  \n",
       "1        True        test  None         2.793035  \n",
       "2        True        test  None         2.853761  \n",
       "3        True  validation  None         2.001374  \n",
       "4        True  validation  None         2.406545  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30698471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('deepseek_v3_predict_c3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
