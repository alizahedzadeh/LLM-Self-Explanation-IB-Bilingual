{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27a9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM (replace with your API key)\n",
    "llm = ChatOpenAI(api_key=\"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\", model=\"gpt-4.1-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d561f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    verbose_explanation: str\n",
    "    concise_explanation: str\n",
    "    masked_verbose: str\n",
    "    masked_concise: str\n",
    "    sufficiency_score_verbose: float\n",
    "    sufficiency_score_concise: float\n",
    "    token_reduction: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642fce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def generate_verbose(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Explain your reasoning step by step for this MCQA question: {question}\"\n",
    "    )\n",
    "    response = llm.invoke(prompt.format(question=state[\"question\"]))\n",
    "    state[\"verbose_explanation\"] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85dc767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_concisely(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Rewrite this explanation more concisely without losing key points: {verbose_explanation}\"\n",
    "    )\n",
    "    response = llm.invoke(prompt.format(verbose_explanation=state[\"verbose_explanation\"]))\n",
    "    state[\"concise_explanation\"] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7177e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_explanations(state: State) -> State:\n",
    "    answer_options = [\"A\", \"B\", \"C\", \"D\"]  # Example options; customize as needed\n",
    "    def mask_text(text):\n",
    "        for option in answer_options:\n",
    "            text = text.replace(option, f\"[Option {option}]\")\n",
    "        return text\n",
    "    state[\"masked_verbose\"] = mask_text(state[\"verbose_explanation\"])\n",
    "    state[\"masked_concise\"] = mask_text(state[\"concise_explanation\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c0d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder scorer (replace with your actual model, e.g., from Hugging Face)\n",
    "def dummy_scorer(masked_explanation, question):\n",
    "    # Simulate scoring; returns confidence in correct answer\n",
    "    return 0.9  # Replace with real scorer output\n",
    "\n",
    "def test_sufficiency(state: State) -> State:\n",
    "    state[\"sufficiency_score_verbose\"] = dummy_scorer(state[\"masked_verbose\"], state[\"question\"])\n",
    "    state[\"sufficiency_score_concise\"] = dummy_scorer(state[\"masked_concise\"], state[\"question\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3aacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Adjust based on your model\n",
    "\n",
    "def measure_token_reduction(state: State) -> State:\n",
    "    verbose_tokens = len(tokenizer.tokenize(state[\"verbose_explanation\"]))\n",
    "    concise_tokens = len(tokenizer.tokenize(state[\"concise_explanation\"]))\n",
    "    state[\"token_reduction\"] = (verbose_tokens - concise_tokens) / verbose_tokens * 100  # Percentage\n",
    "    return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
