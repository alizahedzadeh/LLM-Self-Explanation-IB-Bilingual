{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c2f09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top tokens returned by the model:\n",
      "Token: 'C' | Logprob: -0.1405\n",
      "Token: 'Hello' | Logprob: -2.3905\n",
      "Token: 'The' | Logprob: -3.6405\n",
      "Token: 'To' | Logprob: -4.8905\n",
      "Token: 'B' | Logprob: -5.6405\n",
      "Token: 'It' | Logprob: -6.3905\n",
      "Token: 'D' | Logprob: -9.2655\n",
      "Token: 'Without' | Logprob: -9.2655\n",
      "Token: 'Please' | Logprob: -9.3905\n",
      "Token: 'Could' | Logprob: -9.7655\n",
      "\n",
      "‚úÖ Probabilities (P(Y | Z, Q)):\n",
      "A: 0.0000\n",
      "B: 0.0041\n",
      "C: 0.9958\n",
      "D: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "# Set your API key here\n",
    "api_key = \"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\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "def score_with_logit_bias(question, options, explanation, model=\"gpt-4o\"):\n",
    "    option_letters = list(options.keys())\n",
    "\n",
    "    # Hardcoded token IDs for 'A', 'B', 'C', 'D' based on OpenAI tokenizer (tiktoken)\n",
    "    token_ids = {\n",
    "        \"A\": 64,\n",
    "        \"B\": 66,\n",
    "        \"C\": 68,\n",
    "        \"D\": 70\n",
    "    }\n",
    "\n",
    "    # Only boost those tokens\n",
    "    logit_bias = {str(token_ids[opt]): 100 for opt in option_letters}\n",
    "\n",
    "    # Add newline after \"Answer:\" to ensure clean token boundaries\n",
    "    full_prompt = (\n",
    "        f\"{explanation}\\n\\n\"\n",
    "        f\"Options:\\n\"\n",
    "        + \"\\n\".join([f\"{opt}) {options[opt]}\" for opt in option_letters])\n",
    "        + \"\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            max_tokens=1,\n",
    "            temperature=0.0,\n",
    "            logit_bias=logit_bias,\n",
    "            top_logprobs=10,\n",
    "            logprobs=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"üö® API error: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        top_tokens = response.choices[0].logprobs.content[0].top_logprobs\n",
    "        print(\"\\nüîç Top tokens returned by the model:\")\n",
    "        for entry in top_tokens:\n",
    "            print(f\"Token: '{entry.token}' | Logprob: {entry.logprob:.4f}\")\n",
    "\n",
    "        # Extract logprobs for A, B, C, D\n",
    "        logprobs_dict = {entry.token.strip(): entry.logprob for entry in top_tokens}\n",
    "        selected_logits = [logprobs_dict.get(opt, -100) for opt in option_letters]\n",
    "        probs = softmax(np.array(selected_logits))\n",
    "        return dict(zip(option_letters, probs))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Parsing error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is the primary reason plants need sunlight?\"\n",
    "    options = {\n",
    "        \"A\": \"To stay warm during winter\",\n",
    "        \"B\": \"To perform photosynthesis\",\n",
    "        \"C\": \"To attract pollinators\",\n",
    "        \"D\": \"To conserve water\"\n",
    "    }\n",
    "    explanation = (\n",
    "        \"Hello\"\n",
    "    )\n",
    "\n",
    "    result = score_with_logit_bias(question, options, explanation)\n",
    "\n",
    "    if result:\n",
    "        print(\"\\n‚úÖ Probabilities (P(Y | Z, Q)):\")\n",
    "        for opt, prob in result.items():\n",
    "            print(f\"{opt}: {prob:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Scoring failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb2610bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': -0.4986923038959503, 'A': -0.9986922740936279, 'Based': -3.748692274093628, 'Answer': -7.373692512512207}\n",
      "\n",
      "‚úÖ Intent-based sufficiency (P(option | Z)): \n",
      "A: 1.0000\n",
      "B: 0.0000\n",
      "C: 0.0000\n",
      "D: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = \"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\"\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Hardcoded token IDs for 'A', 'B', 'C', 'D'\n",
    "token_ids = {\n",
    "    \"A\": 64,\n",
    "    \"B\": 66,\n",
    "    \"C\": 68,\n",
    "    \"D\": 70\n",
    "}\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "def score_explanation_intent_based(question, options, explanation, model=\"gpt-4o-mini\"):\n",
    "    option_letters = list(options.keys())\n",
    "\n",
    "    logit_bias = {str(token_ids[opt]): 100 for opt in option_letters}  # only allow A/B/C/D\n",
    "\n",
    "    # Build intent-aligned prompt\n",
    "    full_prompt = (\n",
    "        f\"Options:\\n\" +\n",
    "        \"\\n\".join([f\"{k}) {v}\" for k, v in options.items()]) +\n",
    "        f\"\\n\\nExplanation: {explanation}\\n\\n\"\n",
    "        \"Based only on the explanation above, and assuming it was written by a model selecting the correct answer,\\n\"\n",
    "        \"which option do you think the explanation supports?\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            max_tokens=1,\n",
    "            temperature=0.0,\n",
    "            logit_bias=logit_bias,\n",
    "            top_logprobs=4,\n",
    "            logprobs=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"üö® API error: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        top_tokens = response.choices[0].logprobs.content[0].top_logprobs\n",
    "        logprobs_dict = {entry.token.strip(): entry.logprob for entry in top_tokens}\n",
    "        print(logprobs_dict)\n",
    "        # Pull logprobs for A, B, C, D\n",
    "        selected_logits = [logprobs_dict.get(opt, -100) for opt in option_letters]\n",
    "        probs = softmax(np.array(selected_logits))\n",
    "        return dict(zip(option_letters, probs))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Parsing error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# üîç Example use\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is the primary reason plants need sunlight?\"\n",
    "    options = {\n",
    "        \"A\": \"To stay warm during winter\",\n",
    "        \"B\": \"To perform photosynthesis\",\n",
    "        \"C\": \"To attract pollinators\",\n",
    "        \"D\": \"To conserve water\"\n",
    "    }\n",
    "    explanation = (\n",
    "        \"Sunlight is necessary for the process of To stay warm during winter\"\n",
    "    )\n",
    "\n",
    "    result = score_explanation_intent_based(question, options, explanation)\n",
    "    if result:\n",
    "        print(\"\\n‚úÖ Intent-based sufficiency (P(option | Z)): \")\n",
    "        for opt, prob in result.items():\n",
    "            print(f\"{opt}: {prob:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Scoring failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "549e69e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": null,\n",
      "  \"choices\": null,\n",
      "  \"created\": null,\n",
      "  \"model\": null,\n",
      "  \"object\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": null,\n",
      "  \"error\": {\n",
      "    \"message\": \"Internal Server Error\",\n",
      "    \"code\": 500\n",
      "  },\n",
      "  \"user_id\": \"user_2pktJUyNS9pl1Ldi2CdkQEDjx2I\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"Explanation: Sunlight is necessary for photosynthesis. This process converts light into energy for growth.\n",
    "\n",
    "Question: What is the primary reason plants need sunlight?\n",
    "\n",
    "Options:\n",
    "A) To stay warm during winter\n",
    "B) To perform photosynthesis\n",
    "C) To attract pollinators\n",
    "D) To conserve water\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model=\"mistralai/devstral-small:free\",  # or try \"text-davinci-003\"\n",
    "    prompt=prompt,\n",
    "    max_tokens=0,\n",
    "    temperature=0.0,\n",
    "    echo=True,\n",
    "    logprobs=5\n",
    ")\n",
    "\n",
    "# Print raw response as formatted JSON\n",
    "print(json.dumps(response.model_dump(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5574d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key is valid!\n",
      "ChatCompletion(id='gen-1747929670-TZn9ILlA8fLyRoL7vvky', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='length')], created=1747929670, model='mistralai/devstral-small:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=4, total_tokens=5, completion_tokens_details=None, prompt_tokens_details=None), provider='Chutes')\n",
      "Model responded with token count: 5\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenRouter API key\n",
    "api_key = \"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\"\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Send a harmless minimal request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/devstral-small:free\",  # lightweight model\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=1\n",
    "    )\n",
    "    print(\"‚úÖ API key is valid!\")\n",
    "    print(response)\n",
    "    print(f\"Model responded with token count: {response.usage.total_tokens}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Invalid API key or request error.\")\n",
    "    print(\"Details:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "591011b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model qwen/qwen3-235b-a22b:free supports logprobs (via content).\n",
      "‚ö†Ô∏è No answer option tokens matched.\n",
      "‚úÖ Model mistralai/devstral-small:free supports logprobs (via content).\n",
      "üîç Normalized Probabilities (P(option | explanation)):\n",
      "A: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "def softmax(logits):\n",
    "    logits = np.array(logits)\n",
    "    exp = np.exp(logits - np.max(logits))\n",
    "    return exp / exp.sum()\n",
    "\n",
    "def test_logprob_support(model):\n",
    "    try:\n",
    "        # Full explanation-based MCQA prompt\n",
    "        prompt = \"\"\"Explanation: A\n",
    "\n",
    "Options:\n",
    "A) To stay warm during winter\n",
    "B) To perform photosynthesis\n",
    "C) To attract pollinators\n",
    "D) To conserve water\n",
    "\n",
    "Just say as answer A/B/C/D\"\"\"\n",
    "\n",
    "        # Request with echo=True and logprobs\n",
    "        response = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=5,\n",
    "            logprobs=True,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        logprobs = response.choices[0].logprobs\n",
    "        content = logprobs.content if logprobs else None\n",
    "\n",
    "        if not content:\n",
    "            print(f\"‚ùå Model {model} does NOT return logprobs in content field.\")\n",
    "            return\n",
    "\n",
    "        print(f\"‚úÖ Model {model} supports logprobs (via content).\")\n",
    "\n",
    "        # Extract token logprobs\n",
    "        option_bytes = {\"A\": 65, \"B\": 66, \"C\": 67, \"D\": 68}\n",
    "        scores = {}\n",
    "\n",
    "        for entry in content:\n",
    "            decoded = bytes(entry[\"bytes\"]).decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "            for opt, byte_val in option_bytes.items():\n",
    "                if entry[\"bytes\"] == [byte_val] or decoded == opt:\n",
    "                    scores[opt] = entry[\"logprob\"]\n",
    "\n",
    "        if not scores:\n",
    "            print(\"‚ö†Ô∏è No answer option tokens matched.\")\n",
    "            return\n",
    "\n",
    "        # Normalize\n",
    "        ordered_opts = sorted(scores)\n",
    "        logits = [scores[o] for o in ordered_opts]\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        print(\"üîç Normalized Probabilities (P(option | explanation)):\")\n",
    "        for opt, prob in zip(ordered_opts, probs):\n",
    "            print(f\"{opt}: {prob:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Request failed for model {model}.\")\n",
    "        print(\"Details:\", e)\n",
    "\n",
    "# Test one or more models here\n",
    "test_logprob_support(\"qwen/qwen3-235b-a22b:free\")\n",
    "test_logprob_support(\"mistralai/devstral-small:free\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
